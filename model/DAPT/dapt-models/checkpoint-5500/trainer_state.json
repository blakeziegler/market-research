{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9632059263226007,
  "eval_steps": 500,
  "global_step": 5500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003570073855902894,
      "grad_norm": 0.5501525402069092,
      "learning_rate": 4.9919700214132764e-05,
      "loss": 3.9298,
      "step": 10
    },
    {
      "epoch": 0.007140147711805788,
      "grad_norm": 0.7155733704566956,
      "learning_rate": 4.9830478229835834e-05,
      "loss": 3.8396,
      "step": 20
    },
    {
      "epoch": 0.010710221567708683,
      "grad_norm": 1.1137343645095825,
      "learning_rate": 4.9741256245538904e-05,
      "loss": 3.8701,
      "step": 30
    },
    {
      "epoch": 0.014280295423611577,
      "grad_norm": 0.9838335514068604,
      "learning_rate": 4.965203426124197e-05,
      "loss": 3.5448,
      "step": 40
    },
    {
      "epoch": 0.01785036927951447,
      "grad_norm": 1.2564526796340942,
      "learning_rate": 4.9562812276945044e-05,
      "loss": 3.6699,
      "step": 50
    },
    {
      "epoch": 0.021420443135417366,
      "grad_norm": 1.1866796016693115,
      "learning_rate": 4.947359029264811e-05,
      "loss": 3.6849,
      "step": 60
    },
    {
      "epoch": 0.02499051699132026,
      "grad_norm": 1.3324174880981445,
      "learning_rate": 4.9384368308351184e-05,
      "loss": 3.3296,
      "step": 70
    },
    {
      "epoch": 0.028560590847223153,
      "grad_norm": 1.542923092842102,
      "learning_rate": 4.929514632405425e-05,
      "loss": 3.4109,
      "step": 80
    },
    {
      "epoch": 0.03213066470312605,
      "grad_norm": 1.564994215965271,
      "learning_rate": 4.920592433975732e-05,
      "loss": 3.5207,
      "step": 90
    },
    {
      "epoch": 0.03570073855902894,
      "grad_norm": 1.411889672279358,
      "learning_rate": 4.911670235546039e-05,
      "loss": 3.1852,
      "step": 100
    },
    {
      "epoch": 0.039270812414931834,
      "grad_norm": 1.820064902305603,
      "learning_rate": 4.902748037116346e-05,
      "loss": 3.3964,
      "step": 110
    },
    {
      "epoch": 0.04284088627083473,
      "grad_norm": 1.6011592149734497,
      "learning_rate": 4.893825838686653e-05,
      "loss": 3.2009,
      "step": 120
    },
    {
      "epoch": 0.04641096012673762,
      "grad_norm": 1.744728922843933,
      "learning_rate": 4.88490364025696e-05,
      "loss": 3.3726,
      "step": 130
    },
    {
      "epoch": 0.04998103398264052,
      "grad_norm": 1.7997771501541138,
      "learning_rate": 4.875981441827267e-05,
      "loss": 3.156,
      "step": 140
    },
    {
      "epoch": 0.05355110783854341,
      "grad_norm": 1.67197847366333,
      "learning_rate": 4.867059243397573e-05,
      "loss": 3.0538,
      "step": 150
    },
    {
      "epoch": 0.057121181694446306,
      "grad_norm": 2.804091691970825,
      "learning_rate": 4.85813704496788e-05,
      "loss": 3.2089,
      "step": 160
    },
    {
      "epoch": 0.060691255550349196,
      "grad_norm": 2.1070239543914795,
      "learning_rate": 4.849214846538187e-05,
      "loss": 3.059,
      "step": 170
    },
    {
      "epoch": 0.0642613294062521,
      "grad_norm": 1.7118964195251465,
      "learning_rate": 4.840292648108494e-05,
      "loss": 3.1833,
      "step": 180
    },
    {
      "epoch": 0.06783140326215499,
      "grad_norm": 2.053126573562622,
      "learning_rate": 4.831370449678801e-05,
      "loss": 2.9994,
      "step": 190
    },
    {
      "epoch": 0.07140147711805787,
      "grad_norm": 2.1085500717163086,
      "learning_rate": 4.822448251249108e-05,
      "loss": 3.3006,
      "step": 200
    },
    {
      "epoch": 0.07497155097396077,
      "grad_norm": 1.7055742740631104,
      "learning_rate": 4.813526052819415e-05,
      "loss": 2.9857,
      "step": 210
    },
    {
      "epoch": 0.07854162482986367,
      "grad_norm": 1.6965962648391724,
      "learning_rate": 4.804603854389722e-05,
      "loss": 3.0972,
      "step": 220
    },
    {
      "epoch": 0.08211169868576657,
      "grad_norm": 2.095507860183716,
      "learning_rate": 4.7956816559600284e-05,
      "loss": 3.1388,
      "step": 230
    },
    {
      "epoch": 0.08568177254166946,
      "grad_norm": 2.510179042816162,
      "learning_rate": 4.7876516773733046e-05,
      "loss": 3.2542,
      "step": 240
    },
    {
      "epoch": 0.08925184639757235,
      "grad_norm": 2.213716983795166,
      "learning_rate": 4.778729478943612e-05,
      "loss": 3.23,
      "step": 250
    },
    {
      "epoch": 0.09282192025347524,
      "grad_norm": 2.236377239227295,
      "learning_rate": 4.7698072805139186e-05,
      "loss": 2.991,
      "step": 260
    },
    {
      "epoch": 0.09639199410937814,
      "grad_norm": 2.0323517322540283,
      "learning_rate": 4.760885082084226e-05,
      "loss": 3.0338,
      "step": 270
    },
    {
      "epoch": 0.09996206796528104,
      "grad_norm": 1.9647834300994873,
      "learning_rate": 4.7519628836545326e-05,
      "loss": 3.0287,
      "step": 280
    },
    {
      "epoch": 0.10353214182118392,
      "grad_norm": 2.233867645263672,
      "learning_rate": 4.7430406852248396e-05,
      "loss": 2.9491,
      "step": 290
    },
    {
      "epoch": 0.10710221567708682,
      "grad_norm": 2.0171120166778564,
      "learning_rate": 4.7341184867951466e-05,
      "loss": 2.9887,
      "step": 300
    },
    {
      "epoch": 0.11067228953298971,
      "grad_norm": 2.53202223777771,
      "learning_rate": 4.7251962883654536e-05,
      "loss": 2.9942,
      "step": 310
    },
    {
      "epoch": 0.11424236338889261,
      "grad_norm": 1.9341270923614502,
      "learning_rate": 4.7162740899357606e-05,
      "loss": 2.9994,
      "step": 320
    },
    {
      "epoch": 0.1178124372447955,
      "grad_norm": 2.5503122806549072,
      "learning_rate": 4.707351891506067e-05,
      "loss": 2.9689,
      "step": 330
    },
    {
      "epoch": 0.12138251110069839,
      "grad_norm": 2.0326602458953857,
      "learning_rate": 4.6984296930763746e-05,
      "loss": 3.1694,
      "step": 340
    },
    {
      "epoch": 0.12495258495660129,
      "grad_norm": 1.9281660318374634,
      "learning_rate": 4.689507494646681e-05,
      "loss": 2.9136,
      "step": 350
    },
    {
      "epoch": 0.1285226588125042,
      "grad_norm": 2.3579957485198975,
      "learning_rate": 4.6805852962169886e-05,
      "loss": 2.8653,
      "step": 360
    },
    {
      "epoch": 0.13209273266840707,
      "grad_norm": 2.653787136077881,
      "learning_rate": 4.671663097787295e-05,
      "loss": 2.9317,
      "step": 370
    },
    {
      "epoch": 0.13566280652430998,
      "grad_norm": 2.433443307876587,
      "learning_rate": 4.662740899357602e-05,
      "loss": 2.8115,
      "step": 380
    },
    {
      "epoch": 0.13923288038021286,
      "grad_norm": 2.5302581787109375,
      "learning_rate": 4.653818700927909e-05,
      "loss": 2.9332,
      "step": 390
    },
    {
      "epoch": 0.14280295423611575,
      "grad_norm": 2.60410213470459,
      "learning_rate": 4.644896502498215e-05,
      "loss": 2.92,
      "step": 400
    },
    {
      "epoch": 0.14637302809201866,
      "grad_norm": 2.2327938079833984,
      "learning_rate": 4.635974304068523e-05,
      "loss": 3.0893,
      "step": 410
    },
    {
      "epoch": 0.14994310194792154,
      "grad_norm": 2.0263900756835938,
      "learning_rate": 4.627052105638829e-05,
      "loss": 2.7905,
      "step": 420
    },
    {
      "epoch": 0.15351317580382445,
      "grad_norm": 1.7790433168411255,
      "learning_rate": 4.618129907209137e-05,
      "loss": 2.8729,
      "step": 430
    },
    {
      "epoch": 0.15708324965972734,
      "grad_norm": 2.544670820236206,
      "learning_rate": 4.609207708779443e-05,
      "loss": 2.9292,
      "step": 440
    },
    {
      "epoch": 0.16065332351563022,
      "grad_norm": 2.5086231231689453,
      "learning_rate": 4.60028551034975e-05,
      "loss": 2.7701,
      "step": 450
    },
    {
      "epoch": 0.16422339737153313,
      "grad_norm": 2.328209161758423,
      "learning_rate": 4.591363311920057e-05,
      "loss": 2.9865,
      "step": 460
    },
    {
      "epoch": 0.16779347122743601,
      "grad_norm": 2.7384653091430664,
      "learning_rate": 4.582441113490364e-05,
      "loss": 2.8635,
      "step": 470
    },
    {
      "epoch": 0.17136354508333893,
      "grad_norm": 2.625887870788574,
      "learning_rate": 4.573518915060671e-05,
      "loss": 2.7632,
      "step": 480
    },
    {
      "epoch": 0.1749336189392418,
      "grad_norm": 2.5708789825439453,
      "learning_rate": 4.564596716630978e-05,
      "loss": 2.8031,
      "step": 490
    },
    {
      "epoch": 0.1785036927951447,
      "grad_norm": 2.09214448928833,
      "learning_rate": 4.555674518201285e-05,
      "loss": 2.76,
      "step": 500
    },
    {
      "epoch": 0.1820737666510476,
      "grad_norm": 3.3778436183929443,
      "learning_rate": 4.5467523197715915e-05,
      "loss": 2.7671,
      "step": 510
    },
    {
      "epoch": 0.18564384050695049,
      "grad_norm": 2.5999841690063477,
      "learning_rate": 4.537830121341899e-05,
      "loss": 2.8346,
      "step": 520
    },
    {
      "epoch": 0.18921391436285337,
      "grad_norm": 2.398223876953125,
      "learning_rate": 4.5289079229122055e-05,
      "loss": 2.719,
      "step": 530
    },
    {
      "epoch": 0.19278398821875628,
      "grad_norm": 2.3007118701934814,
      "learning_rate": 4.5199857244825125e-05,
      "loss": 2.8764,
      "step": 540
    },
    {
      "epoch": 0.19635406207465916,
      "grad_norm": 2.9656965732574463,
      "learning_rate": 4.5110635260528195e-05,
      "loss": 2.9006,
      "step": 550
    },
    {
      "epoch": 0.19992413593056207,
      "grad_norm": 2.509348154067993,
      "learning_rate": 4.5021413276231265e-05,
      "loss": 2.6906,
      "step": 560
    },
    {
      "epoch": 0.20349420978646496,
      "grad_norm": 2.8687219619750977,
      "learning_rate": 4.4932191291934335e-05,
      "loss": 2.9608,
      "step": 570
    },
    {
      "epoch": 0.20706428364236784,
      "grad_norm": 3.3556902408599854,
      "learning_rate": 4.4842969307637406e-05,
      "loss": 2.9188,
      "step": 580
    },
    {
      "epoch": 0.21063435749827075,
      "grad_norm": 2.5514371395111084,
      "learning_rate": 4.4753747323340476e-05,
      "loss": 2.8201,
      "step": 590
    },
    {
      "epoch": 0.21420443135417364,
      "grad_norm": 2.187858819961548,
      "learning_rate": 4.4664525339043546e-05,
      "loss": 2.6519,
      "step": 600
    },
    {
      "epoch": 0.21777450521007655,
      "grad_norm": 2.190161943435669,
      "learning_rate": 4.457530335474661e-05,
      "loss": 2.7103,
      "step": 610
    },
    {
      "epoch": 0.22134457906597943,
      "grad_norm": 2.5094833374023438,
      "learning_rate": 4.448608137044968e-05,
      "loss": 2.6968,
      "step": 620
    },
    {
      "epoch": 0.2249146529218823,
      "grad_norm": 2.4983434677124023,
      "learning_rate": 4.439685938615275e-05,
      "loss": 2.7652,
      "step": 630
    },
    {
      "epoch": 0.22848472677778522,
      "grad_norm": 2.6990785598754883,
      "learning_rate": 4.430763740185582e-05,
      "loss": 2.7014,
      "step": 640
    },
    {
      "epoch": 0.2320548006336881,
      "grad_norm": 3.4663751125335693,
      "learning_rate": 4.421841541755889e-05,
      "loss": 2.5887,
      "step": 650
    },
    {
      "epoch": 0.235624874489591,
      "grad_norm": 3.1890876293182373,
      "learning_rate": 4.412919343326196e-05,
      "loss": 2.9664,
      "step": 660
    },
    {
      "epoch": 0.2391949483454939,
      "grad_norm": 2.4210073947906494,
      "learning_rate": 4.403997144896503e-05,
      "loss": 2.735,
      "step": 670
    },
    {
      "epoch": 0.24276502220139679,
      "grad_norm": 2.1739232540130615,
      "learning_rate": 4.395074946466809e-05,
      "loss": 2.8051,
      "step": 680
    },
    {
      "epoch": 0.2463350960572997,
      "grad_norm": 2.4897665977478027,
      "learning_rate": 4.386152748037117e-05,
      "loss": 2.7335,
      "step": 690
    },
    {
      "epoch": 0.24990516991320258,
      "grad_norm": 2.2108335494995117,
      "learning_rate": 4.377230549607423e-05,
      "loss": 2.6091,
      "step": 700
    },
    {
      "epoch": 0.25347524376910546,
      "grad_norm": 2.7026665210723877,
      "learning_rate": 4.368308351177731e-05,
      "loss": 2.7606,
      "step": 710
    },
    {
      "epoch": 0.2570453176250084,
      "grad_norm": 2.5997540950775146,
      "learning_rate": 4.359386152748037e-05,
      "loss": 2.6612,
      "step": 720
    },
    {
      "epoch": 0.2606153914809113,
      "grad_norm": 2.663120746612549,
      "learning_rate": 4.350463954318344e-05,
      "loss": 2.6284,
      "step": 730
    },
    {
      "epoch": 0.26418546533681414,
      "grad_norm": 3.146914482116699,
      "learning_rate": 4.341541755888651e-05,
      "loss": 2.7162,
      "step": 740
    },
    {
      "epoch": 0.26775553919271705,
      "grad_norm": 3.0308029651641846,
      "learning_rate": 4.3326195574589575e-05,
      "loss": 2.5954,
      "step": 750
    },
    {
      "epoch": 0.27132561304861996,
      "grad_norm": 2.4209492206573486,
      "learning_rate": 4.323697359029265e-05,
      "loss": 2.8511,
      "step": 760
    },
    {
      "epoch": 0.2748956869045228,
      "grad_norm": 2.823371171951294,
      "learning_rate": 4.3147751605995715e-05,
      "loss": 2.6967,
      "step": 770
    },
    {
      "epoch": 0.27846576076042573,
      "grad_norm": 2.9007558822631836,
      "learning_rate": 4.305852962169879e-05,
      "loss": 2.6236,
      "step": 780
    },
    {
      "epoch": 0.28203583461632864,
      "grad_norm": 2.5390138626098633,
      "learning_rate": 4.2969307637401855e-05,
      "loss": 2.6212,
      "step": 790
    },
    {
      "epoch": 0.2856059084722315,
      "grad_norm": 2.529374361038208,
      "learning_rate": 4.288008565310493e-05,
      "loss": 2.7812,
      "step": 800
    },
    {
      "epoch": 0.2891759823281344,
      "grad_norm": 2.3528082370758057,
      "learning_rate": 4.2790863668807995e-05,
      "loss": 2.7014,
      "step": 810
    },
    {
      "epoch": 0.2927460561840373,
      "grad_norm": 2.370478391647339,
      "learning_rate": 4.2701641684511065e-05,
      "loss": 2.5777,
      "step": 820
    },
    {
      "epoch": 0.29631613003994023,
      "grad_norm": 3.148350954055786,
      "learning_rate": 4.2612419700214135e-05,
      "loss": 2.652,
      "step": 830
    },
    {
      "epoch": 0.2998862038958431,
      "grad_norm": 3.0646955966949463,
      "learning_rate": 4.2523197715917205e-05,
      "loss": 2.7971,
      "step": 840
    },
    {
      "epoch": 0.303456277751746,
      "grad_norm": 2.463899612426758,
      "learning_rate": 4.2433975731620275e-05,
      "loss": 2.6567,
      "step": 850
    },
    {
      "epoch": 0.3070263516076489,
      "grad_norm": 2.5872275829315186,
      "learning_rate": 4.234475374732334e-05,
      "loss": 2.5633,
      "step": 860
    },
    {
      "epoch": 0.31059642546355176,
      "grad_norm": 3.0014398097991943,
      "learning_rate": 4.2255531763026415e-05,
      "loss": 2.7901,
      "step": 870
    },
    {
      "epoch": 0.3141664993194547,
      "grad_norm": 3.171481132507324,
      "learning_rate": 4.216630977872948e-05,
      "loss": 2.61,
      "step": 880
    },
    {
      "epoch": 0.3177365731753576,
      "grad_norm": 2.1594741344451904,
      "learning_rate": 4.207708779443255e-05,
      "loss": 2.6592,
      "step": 890
    },
    {
      "epoch": 0.32130664703126044,
      "grad_norm": 2.7784698009490967,
      "learning_rate": 4.198786581013562e-05,
      "loss": 2.5531,
      "step": 900
    },
    {
      "epoch": 0.32487672088716335,
      "grad_norm": 3.043722629547119,
      "learning_rate": 4.189864382583869e-05,
      "loss": 2.6159,
      "step": 910
    },
    {
      "epoch": 0.32844679474306626,
      "grad_norm": 2.5654196739196777,
      "learning_rate": 4.180942184154176e-05,
      "loss": 2.6172,
      "step": 920
    },
    {
      "epoch": 0.3320168685989691,
      "grad_norm": 2.871180534362793,
      "learning_rate": 4.172019985724483e-05,
      "loss": 2.6755,
      "step": 930
    },
    {
      "epoch": 0.33558694245487203,
      "grad_norm": 3.084362506866455,
      "learning_rate": 4.16309778729479e-05,
      "loss": 2.5291,
      "step": 940
    },
    {
      "epoch": 0.33915701631077494,
      "grad_norm": 2.724677801132202,
      "learning_rate": 4.154175588865097e-05,
      "loss": 2.8844,
      "step": 950
    },
    {
      "epoch": 0.34272709016667785,
      "grad_norm": 3.2610373497009277,
      "learning_rate": 4.145253390435403e-05,
      "loss": 2.7222,
      "step": 960
    },
    {
      "epoch": 0.3462971640225807,
      "grad_norm": 2.7645797729492188,
      "learning_rate": 4.13633119200571e-05,
      "loss": 2.6255,
      "step": 970
    },
    {
      "epoch": 0.3498672378784836,
      "grad_norm": 2.704763650894165,
      "learning_rate": 4.127408993576017e-05,
      "loss": 2.709,
      "step": 980
    },
    {
      "epoch": 0.35343731173438653,
      "grad_norm": 3.513657569885254,
      "learning_rate": 4.118486795146324e-05,
      "loss": 2.5723,
      "step": 990
    },
    {
      "epoch": 0.3570073855902894,
      "grad_norm": 3.1728029251098633,
      "learning_rate": 4.109564596716631e-05,
      "loss": 2.6035,
      "step": 1000
    },
    {
      "epoch": 0.3605774594461923,
      "grad_norm": 3.6226699352264404,
      "learning_rate": 4.100642398286938e-05,
      "loss": 2.5786,
      "step": 1010
    },
    {
      "epoch": 0.3641475333020952,
      "grad_norm": 3.014285087585449,
      "learning_rate": 4.091720199857245e-05,
      "loss": 2.5945,
      "step": 1020
    },
    {
      "epoch": 0.36771760715799806,
      "grad_norm": 3.1824424266815186,
      "learning_rate": 4.082798001427552e-05,
      "loss": 2.6982,
      "step": 1030
    },
    {
      "epoch": 0.37128768101390097,
      "grad_norm": 2.4492299556732178,
      "learning_rate": 4.073875802997859e-05,
      "loss": 2.7473,
      "step": 1040
    },
    {
      "epoch": 0.3748577548698039,
      "grad_norm": 2.7707343101501465,
      "learning_rate": 4.0649536045681655e-05,
      "loss": 2.5109,
      "step": 1050
    },
    {
      "epoch": 0.37842782872570674,
      "grad_norm": 3.062866687774658,
      "learning_rate": 4.056031406138473e-05,
      "loss": 2.6231,
      "step": 1060
    },
    {
      "epoch": 0.38199790258160965,
      "grad_norm": 2.5948214530944824,
      "learning_rate": 4.0471092077087795e-05,
      "loss": 2.5488,
      "step": 1070
    },
    {
      "epoch": 0.38556797643751256,
      "grad_norm": 3.2454733848571777,
      "learning_rate": 4.0381870092790865e-05,
      "loss": 2.4971,
      "step": 1080
    },
    {
      "epoch": 0.38913805029341547,
      "grad_norm": 3.050210952758789,
      "learning_rate": 4.0292648108493935e-05,
      "loss": 2.6198,
      "step": 1090
    },
    {
      "epoch": 0.3927081241493183,
      "grad_norm": 2.8769874572753906,
      "learning_rate": 4.0203426124197005e-05,
      "loss": 2.6203,
      "step": 1100
    },
    {
      "epoch": 0.39627819800522124,
      "grad_norm": 3.3690297603607178,
      "learning_rate": 4.0114204139900075e-05,
      "loss": 2.6914,
      "step": 1110
    },
    {
      "epoch": 0.39984827186112415,
      "grad_norm": 3.230649709701538,
      "learning_rate": 4.002498215560314e-05,
      "loss": 2.5562,
      "step": 1120
    },
    {
      "epoch": 0.403418345717027,
      "grad_norm": 2.400785207748413,
      "learning_rate": 3.9935760171306215e-05,
      "loss": 2.5228,
      "step": 1130
    },
    {
      "epoch": 0.4069884195729299,
      "grad_norm": 2.991236448287964,
      "learning_rate": 3.984653818700928e-05,
      "loss": 2.5848,
      "step": 1140
    },
    {
      "epoch": 0.4105584934288328,
      "grad_norm": 2.912964105606079,
      "learning_rate": 3.9757316202712355e-05,
      "loss": 2.5448,
      "step": 1150
    },
    {
      "epoch": 0.4141285672847357,
      "grad_norm": 2.5962154865264893,
      "learning_rate": 3.966809421841542e-05,
      "loss": 2.5973,
      "step": 1160
    },
    {
      "epoch": 0.4176986411406386,
      "grad_norm": 2.9038140773773193,
      "learning_rate": 3.957887223411849e-05,
      "loss": 2.5636,
      "step": 1170
    },
    {
      "epoch": 0.4212687149965415,
      "grad_norm": 2.976130485534668,
      "learning_rate": 3.948965024982156e-05,
      "loss": 2.7628,
      "step": 1180
    },
    {
      "epoch": 0.42483878885244436,
      "grad_norm": 3.61541748046875,
      "learning_rate": 3.940042826552462e-05,
      "loss": 2.7869,
      "step": 1190
    },
    {
      "epoch": 0.42840886270834727,
      "grad_norm": 2.4073526859283447,
      "learning_rate": 3.93112062812277e-05,
      "loss": 2.5929,
      "step": 1200
    },
    {
      "epoch": 0.4319789365642502,
      "grad_norm": 2.9577221870422363,
      "learning_rate": 3.922198429693076e-05,
      "loss": 2.6201,
      "step": 1210
    },
    {
      "epoch": 0.4355490104201531,
      "grad_norm": 2.9798879623413086,
      "learning_rate": 3.913276231263384e-05,
      "loss": 2.7981,
      "step": 1220
    },
    {
      "epoch": 0.43911908427605595,
      "grad_norm": 4.012415885925293,
      "learning_rate": 3.90435403283369e-05,
      "loss": 2.6448,
      "step": 1230
    },
    {
      "epoch": 0.44268915813195886,
      "grad_norm": 3.773188829421997,
      "learning_rate": 3.895431834403997e-05,
      "loss": 2.722,
      "step": 1240
    },
    {
      "epoch": 0.44625923198786177,
      "grad_norm": 2.3163628578186035,
      "learning_rate": 3.886509635974304e-05,
      "loss": 2.6314,
      "step": 1250
    },
    {
      "epoch": 0.4498293058437646,
      "grad_norm": 3.242220401763916,
      "learning_rate": 3.877587437544611e-05,
      "loss": 2.4476,
      "step": 1260
    },
    {
      "epoch": 0.45339937969966754,
      "grad_norm": 2.8092408180236816,
      "learning_rate": 3.868665239114918e-05,
      "loss": 2.5738,
      "step": 1270
    },
    {
      "epoch": 0.45696945355557045,
      "grad_norm": 2.91666579246521,
      "learning_rate": 3.859743040685225e-05,
      "loss": 2.5203,
      "step": 1280
    },
    {
      "epoch": 0.4605395274114733,
      "grad_norm": 2.1785387992858887,
      "learning_rate": 3.850820842255532e-05,
      "loss": 2.5647,
      "step": 1290
    },
    {
      "epoch": 0.4641096012673762,
      "grad_norm": 3.3562285900115967,
      "learning_rate": 3.8418986438258385e-05,
      "loss": 2.7136,
      "step": 1300
    },
    {
      "epoch": 0.4676796751232791,
      "grad_norm": 3.0816659927368164,
      "learning_rate": 3.832976445396146e-05,
      "loss": 2.625,
      "step": 1310
    },
    {
      "epoch": 0.471249748979182,
      "grad_norm": 2.704845666885376,
      "learning_rate": 3.8240542469664525e-05,
      "loss": 2.4975,
      "step": 1320
    },
    {
      "epoch": 0.4748198228350849,
      "grad_norm": 2.918344020843506,
      "learning_rate": 3.8151320485367595e-05,
      "loss": 2.679,
      "step": 1330
    },
    {
      "epoch": 0.4783898966909878,
      "grad_norm": 4.039375305175781,
      "learning_rate": 3.8062098501070665e-05,
      "loss": 2.5245,
      "step": 1340
    },
    {
      "epoch": 0.4819599705468907,
      "grad_norm": 2.829582452774048,
      "learning_rate": 3.7972876516773735e-05,
      "loss": 2.6069,
      "step": 1350
    },
    {
      "epoch": 0.48553004440279357,
      "grad_norm": 3.2414565086364746,
      "learning_rate": 3.7883654532476805e-05,
      "loss": 2.6443,
      "step": 1360
    },
    {
      "epoch": 0.4891001182586965,
      "grad_norm": 3.4292380809783936,
      "learning_rate": 3.7794432548179875e-05,
      "loss": 2.5755,
      "step": 1370
    },
    {
      "epoch": 0.4926701921145994,
      "grad_norm": 2.714974880218506,
      "learning_rate": 3.7705210563882945e-05,
      "loss": 2.6886,
      "step": 1380
    },
    {
      "epoch": 0.49624026597050225,
      "grad_norm": 3.06099796295166,
      "learning_rate": 3.7615988579586015e-05,
      "loss": 2.5379,
      "step": 1390
    },
    {
      "epoch": 0.49981033982640516,
      "grad_norm": 3.181049108505249,
      "learning_rate": 3.752676659528908e-05,
      "loss": 2.5042,
      "step": 1400
    },
    {
      "epoch": 0.503380413682308,
      "grad_norm": 2.5113072395324707,
      "learning_rate": 3.743754461099215e-05,
      "loss": 2.5693,
      "step": 1410
    },
    {
      "epoch": 0.5069504875382109,
      "grad_norm": 2.8564817905426025,
      "learning_rate": 3.734832262669522e-05,
      "loss": 2.4157,
      "step": 1420
    },
    {
      "epoch": 0.5105205613941138,
      "grad_norm": 3.1618893146514893,
      "learning_rate": 3.725910064239829e-05,
      "loss": 2.6817,
      "step": 1430
    },
    {
      "epoch": 0.5140906352500167,
      "grad_norm": 3.338132381439209,
      "learning_rate": 3.716987865810136e-05,
      "loss": 2.505,
      "step": 1440
    },
    {
      "epoch": 0.5176607091059197,
      "grad_norm": 3.1742849349975586,
      "learning_rate": 3.708065667380443e-05,
      "loss": 2.6067,
      "step": 1450
    },
    {
      "epoch": 0.5212307829618226,
      "grad_norm": 2.7718582153320312,
      "learning_rate": 3.69914346895075e-05,
      "loss": 2.4176,
      "step": 1460
    },
    {
      "epoch": 0.5248008568177254,
      "grad_norm": 3.1904377937316895,
      "learning_rate": 3.690221270521056e-05,
      "loss": 2.4495,
      "step": 1470
    },
    {
      "epoch": 0.5283709306736283,
      "grad_norm": 2.9411087036132812,
      "learning_rate": 3.681299072091364e-05,
      "loss": 2.6013,
      "step": 1480
    },
    {
      "epoch": 0.5319410045295312,
      "grad_norm": 3.580286741256714,
      "learning_rate": 3.67237687366167e-05,
      "loss": 2.6772,
      "step": 1490
    },
    {
      "epoch": 0.5355110783854341,
      "grad_norm": 2.816173791885376,
      "learning_rate": 3.663454675231978e-05,
      "loss": 2.6096,
      "step": 1500
    },
    {
      "epoch": 0.539081152241337,
      "grad_norm": 3.257910966873169,
      "learning_rate": 3.654532476802284e-05,
      "loss": 2.5472,
      "step": 1510
    },
    {
      "epoch": 0.5426512260972399,
      "grad_norm": 3.578988790512085,
      "learning_rate": 3.645610278372591e-05,
      "loss": 2.5506,
      "step": 1520
    },
    {
      "epoch": 0.5462212999531428,
      "grad_norm": 2.1333024501800537,
      "learning_rate": 3.636688079942898e-05,
      "loss": 2.6324,
      "step": 1530
    },
    {
      "epoch": 0.5497913738090456,
      "grad_norm": 3.1475892066955566,
      "learning_rate": 3.627765881513205e-05,
      "loss": 2.4555,
      "step": 1540
    },
    {
      "epoch": 0.5533614476649485,
      "grad_norm": 2.623915195465088,
      "learning_rate": 3.618843683083512e-05,
      "loss": 2.5292,
      "step": 1550
    },
    {
      "epoch": 0.5569315215208515,
      "grad_norm": 2.504952907562256,
      "learning_rate": 3.6099214846538185e-05,
      "loss": 2.5209,
      "step": 1560
    },
    {
      "epoch": 0.5605015953767544,
      "grad_norm": 2.874661684036255,
      "learning_rate": 3.600999286224126e-05,
      "loss": 2.6438,
      "step": 1570
    },
    {
      "epoch": 0.5640716692326573,
      "grad_norm": 3.4743692874908447,
      "learning_rate": 3.5920770877944325e-05,
      "loss": 2.7197,
      "step": 1580
    },
    {
      "epoch": 0.5676417430885602,
      "grad_norm": 3.861304759979248,
      "learning_rate": 3.58315488936474e-05,
      "loss": 2.482,
      "step": 1590
    },
    {
      "epoch": 0.571211816944463,
      "grad_norm": 2.8527989387512207,
      "learning_rate": 3.5742326909350465e-05,
      "loss": 2.5729,
      "step": 1600
    },
    {
      "epoch": 0.5747818908003659,
      "grad_norm": 3.296710729598999,
      "learning_rate": 3.5653104925053535e-05,
      "loss": 2.5817,
      "step": 1610
    },
    {
      "epoch": 0.5783519646562688,
      "grad_norm": 3.554413318634033,
      "learning_rate": 3.5563882940756605e-05,
      "loss": 2.6466,
      "step": 1620
    },
    {
      "epoch": 0.5819220385121717,
      "grad_norm": 3.680316686630249,
      "learning_rate": 3.547466095645967e-05,
      "loss": 2.6375,
      "step": 1630
    },
    {
      "epoch": 0.5854921123680746,
      "grad_norm": 2.5986649990081787,
      "learning_rate": 3.5385438972162745e-05,
      "loss": 2.6788,
      "step": 1640
    },
    {
      "epoch": 0.5890621862239775,
      "grad_norm": 2.866621255874634,
      "learning_rate": 3.529621698786581e-05,
      "loss": 2.4679,
      "step": 1650
    },
    {
      "epoch": 0.5926322600798805,
      "grad_norm": 3.6162524223327637,
      "learning_rate": 3.5206995003568885e-05,
      "loss": 2.6091,
      "step": 1660
    },
    {
      "epoch": 0.5962023339357833,
      "grad_norm": 3.5580105781555176,
      "learning_rate": 3.511777301927195e-05,
      "loss": 2.4616,
      "step": 1670
    },
    {
      "epoch": 0.5997724077916862,
      "grad_norm": 2.6592822074890137,
      "learning_rate": 3.502855103497502e-05,
      "loss": 2.68,
      "step": 1680
    },
    {
      "epoch": 0.6033424816475891,
      "grad_norm": 2.989837646484375,
      "learning_rate": 3.493932905067809e-05,
      "loss": 2.5202,
      "step": 1690
    },
    {
      "epoch": 0.606912555503492,
      "grad_norm": 3.0219357013702393,
      "learning_rate": 3.485010706638116e-05,
      "loss": 2.5057,
      "step": 1700
    },
    {
      "epoch": 0.6104826293593949,
      "grad_norm": 3.395768642425537,
      "learning_rate": 3.476088508208423e-05,
      "loss": 2.5563,
      "step": 1710
    },
    {
      "epoch": 0.6140527032152978,
      "grad_norm": 2.8742802143096924,
      "learning_rate": 3.46716630977873e-05,
      "loss": 2.5903,
      "step": 1720
    },
    {
      "epoch": 0.6176227770712006,
      "grad_norm": 3.5433032512664795,
      "learning_rate": 3.458244111349037e-05,
      "loss": 2.6148,
      "step": 1730
    },
    {
      "epoch": 0.6211928509271035,
      "grad_norm": 3.088451623916626,
      "learning_rate": 3.449321912919343e-05,
      "loss": 2.6621,
      "step": 1740
    },
    {
      "epoch": 0.6247629247830064,
      "grad_norm": 3.6309335231781006,
      "learning_rate": 3.44039971448965e-05,
      "loss": 2.5621,
      "step": 1750
    },
    {
      "epoch": 0.6283329986389093,
      "grad_norm": 3.3850154876708984,
      "learning_rate": 3.431477516059957e-05,
      "loss": 2.5824,
      "step": 1760
    },
    {
      "epoch": 0.6319030724948123,
      "grad_norm": 2.6545369625091553,
      "learning_rate": 3.422555317630264e-05,
      "loss": 2.6807,
      "step": 1770
    },
    {
      "epoch": 0.6354731463507152,
      "grad_norm": 2.6617724895477295,
      "learning_rate": 3.413633119200571e-05,
      "loss": 2.5768,
      "step": 1780
    },
    {
      "epoch": 0.6390432202066181,
      "grad_norm": 2.797947883605957,
      "learning_rate": 3.404710920770878e-05,
      "loss": 2.4589,
      "step": 1790
    },
    {
      "epoch": 0.6426132940625209,
      "grad_norm": 2.633680820465088,
      "learning_rate": 3.395788722341185e-05,
      "loss": 2.571,
      "step": 1800
    },
    {
      "epoch": 0.6461833679184238,
      "grad_norm": 4.367575645446777,
      "learning_rate": 3.386866523911492e-05,
      "loss": 2.7241,
      "step": 1810
    },
    {
      "epoch": 0.6497534417743267,
      "grad_norm": 3.0624306201934814,
      "learning_rate": 3.377944325481799e-05,
      "loss": 2.4428,
      "step": 1820
    },
    {
      "epoch": 0.6533235156302296,
      "grad_norm": 2.5404281616210938,
      "learning_rate": 3.369022127052106e-05,
      "loss": 2.5182,
      "step": 1830
    },
    {
      "epoch": 0.6568935894861325,
      "grad_norm": 3.0232455730438232,
      "learning_rate": 3.3600999286224125e-05,
      "loss": 2.5824,
      "step": 1840
    },
    {
      "epoch": 0.6604636633420354,
      "grad_norm": 3.3247017860412598,
      "learning_rate": 3.3511777301927195e-05,
      "loss": 2.6609,
      "step": 1850
    },
    {
      "epoch": 0.6640337371979382,
      "grad_norm": 3.476271152496338,
      "learning_rate": 3.3422555317630265e-05,
      "loss": 2.4862,
      "step": 1860
    },
    {
      "epoch": 0.6676038110538411,
      "grad_norm": 3.6858551502227783,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 2.6564,
      "step": 1870
    },
    {
      "epoch": 0.6711738849097441,
      "grad_norm": 3.661940813064575,
      "learning_rate": 3.3244111349036405e-05,
      "loss": 2.556,
      "step": 1880
    },
    {
      "epoch": 0.674743958765647,
      "grad_norm": 3.1314408779144287,
      "learning_rate": 3.3154889364739475e-05,
      "loss": 2.4241,
      "step": 1890
    },
    {
      "epoch": 0.6783140326215499,
      "grad_norm": 3.235788345336914,
      "learning_rate": 3.3065667380442545e-05,
      "loss": 2.5379,
      "step": 1900
    },
    {
      "epoch": 0.6818841064774528,
      "grad_norm": 3.1421210765838623,
      "learning_rate": 3.297644539614561e-05,
      "loss": 2.6505,
      "step": 1910
    },
    {
      "epoch": 0.6854541803333557,
      "grad_norm": 3.3547775745391846,
      "learning_rate": 3.2887223411848685e-05,
      "loss": 2.6422,
      "step": 1920
    },
    {
      "epoch": 0.6890242541892585,
      "grad_norm": 3.2993011474609375,
      "learning_rate": 3.279800142755175e-05,
      "loss": 2.4763,
      "step": 1930
    },
    {
      "epoch": 0.6925943280451614,
      "grad_norm": 4.108704566955566,
      "learning_rate": 3.2708779443254825e-05,
      "loss": 2.4257,
      "step": 1940
    },
    {
      "epoch": 0.6961644019010643,
      "grad_norm": 2.966093063354492,
      "learning_rate": 3.261955745895789e-05,
      "loss": 2.6291,
      "step": 1950
    },
    {
      "epoch": 0.6997344757569672,
      "grad_norm": 3.157383441925049,
      "learning_rate": 3.253033547466096e-05,
      "loss": 2.6076,
      "step": 1960
    },
    {
      "epoch": 0.7033045496128701,
      "grad_norm": 3.5171329975128174,
      "learning_rate": 3.244111349036403e-05,
      "loss": 2.6827,
      "step": 1970
    },
    {
      "epoch": 0.7068746234687731,
      "grad_norm": 3.059706687927246,
      "learning_rate": 3.235189150606709e-05,
      "loss": 2.5072,
      "step": 1980
    },
    {
      "epoch": 0.7104446973246759,
      "grad_norm": 3.156980276107788,
      "learning_rate": 3.226266952177017e-05,
      "loss": 2.4082,
      "step": 1990
    },
    {
      "epoch": 0.7140147711805788,
      "grad_norm": 3.823254346847534,
      "learning_rate": 3.217344753747323e-05,
      "loss": 2.5871,
      "step": 2000
    },
    {
      "epoch": 0.7175848450364817,
      "grad_norm": 3.918428897857666,
      "learning_rate": 3.208422555317631e-05,
      "loss": 2.4734,
      "step": 2010
    },
    {
      "epoch": 0.7211549188923846,
      "grad_norm": 3.6681067943573,
      "learning_rate": 3.199500356887937e-05,
      "loss": 2.5793,
      "step": 2020
    },
    {
      "epoch": 0.7247249927482875,
      "grad_norm": 3.0433881282806396,
      "learning_rate": 3.190578158458244e-05,
      "loss": 2.5006,
      "step": 2030
    },
    {
      "epoch": 0.7282950666041904,
      "grad_norm": 4.025468826293945,
      "learning_rate": 3.181655960028551e-05,
      "loss": 2.4606,
      "step": 2040
    },
    {
      "epoch": 0.7318651404600933,
      "grad_norm": 2.7295985221862793,
      "learning_rate": 3.172733761598858e-05,
      "loss": 2.5274,
      "step": 2050
    },
    {
      "epoch": 0.7354352143159961,
      "grad_norm": 3.9999115467071533,
      "learning_rate": 3.163811563169165e-05,
      "loss": 2.6831,
      "step": 2060
    },
    {
      "epoch": 0.739005288171899,
      "grad_norm": 3.183713674545288,
      "learning_rate": 3.1548893647394715e-05,
      "loss": 2.5741,
      "step": 2070
    },
    {
      "epoch": 0.7425753620278019,
      "grad_norm": 2.8795571327209473,
      "learning_rate": 3.145967166309779e-05,
      "loss": 2.4702,
      "step": 2080
    },
    {
      "epoch": 0.7461454358837049,
      "grad_norm": 2.9325125217437744,
      "learning_rate": 3.1370449678800855e-05,
      "loss": 2.4951,
      "step": 2090
    },
    {
      "epoch": 0.7497155097396078,
      "grad_norm": 2.7025537490844727,
      "learning_rate": 3.128122769450393e-05,
      "loss": 2.5513,
      "step": 2100
    },
    {
      "epoch": 0.7532855835955107,
      "grad_norm": 2.9043328762054443,
      "learning_rate": 3.1192005710206995e-05,
      "loss": 2.4517,
      "step": 2110
    },
    {
      "epoch": 0.7568556574514135,
      "grad_norm": 3.4207050800323486,
      "learning_rate": 3.1102783725910065e-05,
      "loss": 2.608,
      "step": 2120
    },
    {
      "epoch": 0.7604257313073164,
      "grad_norm": 2.9187328815460205,
      "learning_rate": 3.1013561741613135e-05,
      "loss": 2.5867,
      "step": 2130
    },
    {
      "epoch": 0.7639958051632193,
      "grad_norm": 2.723433256149292,
      "learning_rate": 3.0924339757316205e-05,
      "loss": 2.6726,
      "step": 2140
    },
    {
      "epoch": 0.7675658790191222,
      "grad_norm": 4.132968902587891,
      "learning_rate": 3.0835117773019275e-05,
      "loss": 2.5239,
      "step": 2150
    },
    {
      "epoch": 0.7711359528750251,
      "grad_norm": 4.183256149291992,
      "learning_rate": 3.0745895788722345e-05,
      "loss": 2.6032,
      "step": 2160
    },
    {
      "epoch": 0.774706026730928,
      "grad_norm": 4.100472927093506,
      "learning_rate": 3.0656673804425415e-05,
      "loss": 2.6439,
      "step": 2170
    },
    {
      "epoch": 0.7782761005868309,
      "grad_norm": 2.884263515472412,
      "learning_rate": 3.056745182012848e-05,
      "loss": 2.6322,
      "step": 2180
    },
    {
      "epoch": 0.7818461744427337,
      "grad_norm": 3.478318691253662,
      "learning_rate": 3.047822983583155e-05,
      "loss": 2.4665,
      "step": 2190
    },
    {
      "epoch": 0.7854162482986367,
      "grad_norm": 3.9317851066589355,
      "learning_rate": 3.0389007851534618e-05,
      "loss": 2.5174,
      "step": 2200
    },
    {
      "epoch": 0.7889863221545396,
      "grad_norm": 2.7529780864715576,
      "learning_rate": 3.029978586723769e-05,
      "loss": 2.5617,
      "step": 2210
    },
    {
      "epoch": 0.7925563960104425,
      "grad_norm": 3.380530595779419,
      "learning_rate": 3.0210563882940758e-05,
      "loss": 2.6773,
      "step": 2220
    },
    {
      "epoch": 0.7961264698663454,
      "grad_norm": 3.577301025390625,
      "learning_rate": 3.0121341898643828e-05,
      "loss": 2.5183,
      "step": 2230
    },
    {
      "epoch": 0.7996965437222483,
      "grad_norm": 3.0979835987091064,
      "learning_rate": 3.0032119914346895e-05,
      "loss": 2.5833,
      "step": 2240
    },
    {
      "epoch": 0.8032666175781511,
      "grad_norm": 3.008862018585205,
      "learning_rate": 2.9942897930049968e-05,
      "loss": 2.748,
      "step": 2250
    },
    {
      "epoch": 0.806836691434054,
      "grad_norm": 3.459282398223877,
      "learning_rate": 2.9853675945753035e-05,
      "loss": 2.5325,
      "step": 2260
    },
    {
      "epoch": 0.8104067652899569,
      "grad_norm": 3.5350959300994873,
      "learning_rate": 2.9764453961456108e-05,
      "loss": 2.4822,
      "step": 2270
    },
    {
      "epoch": 0.8139768391458598,
      "grad_norm": 2.8248214721679688,
      "learning_rate": 2.9675231977159175e-05,
      "loss": 2.6212,
      "step": 2280
    },
    {
      "epoch": 0.8175469130017627,
      "grad_norm": 2.9620749950408936,
      "learning_rate": 2.958600999286224e-05,
      "loss": 2.481,
      "step": 2290
    },
    {
      "epoch": 0.8211169868576657,
      "grad_norm": 3.8915505409240723,
      "learning_rate": 2.949678800856531e-05,
      "loss": 2.533,
      "step": 2300
    },
    {
      "epoch": 0.8246870607135686,
      "grad_norm": 2.5788772106170654,
      "learning_rate": 2.9407566024268378e-05,
      "loss": 2.474,
      "step": 2310
    },
    {
      "epoch": 0.8282571345694714,
      "grad_norm": 3.461275815963745,
      "learning_rate": 2.931834403997145e-05,
      "loss": 2.4947,
      "step": 2320
    },
    {
      "epoch": 0.8318272084253743,
      "grad_norm": 3.7686715126037598,
      "learning_rate": 2.9229122055674518e-05,
      "loss": 2.507,
      "step": 2330
    },
    {
      "epoch": 0.8353972822812772,
      "grad_norm": 3.0499279499053955,
      "learning_rate": 2.913990007137759e-05,
      "loss": 2.5585,
      "step": 2340
    },
    {
      "epoch": 0.8389673561371801,
      "grad_norm": 2.471097230911255,
      "learning_rate": 2.9050678087080658e-05,
      "loss": 2.5109,
      "step": 2350
    },
    {
      "epoch": 0.842537429993083,
      "grad_norm": 5.154210090637207,
      "learning_rate": 2.8961456102783728e-05,
      "loss": 2.4619,
      "step": 2360
    },
    {
      "epoch": 0.8461075038489859,
      "grad_norm": 4.0974650382995605,
      "learning_rate": 2.8872234118486795e-05,
      "loss": 2.4801,
      "step": 2370
    },
    {
      "epoch": 0.8496775777048887,
      "grad_norm": 2.95654296875,
      "learning_rate": 2.8783012134189868e-05,
      "loss": 2.6311,
      "step": 2380
    },
    {
      "epoch": 0.8532476515607916,
      "grad_norm": 3.7946200370788574,
      "learning_rate": 2.8693790149892935e-05,
      "loss": 2.4764,
      "step": 2390
    },
    {
      "epoch": 0.8568177254166945,
      "grad_norm": 3.0517141819000244,
      "learning_rate": 2.8604568165596e-05,
      "loss": 2.6729,
      "step": 2400
    },
    {
      "epoch": 0.8603877992725975,
      "grad_norm": 3.0458571910858154,
      "learning_rate": 2.8515346181299075e-05,
      "loss": 2.5207,
      "step": 2410
    },
    {
      "epoch": 0.8639578731285004,
      "grad_norm": 2.60040283203125,
      "learning_rate": 2.842612419700214e-05,
      "loss": 2.5238,
      "step": 2420
    },
    {
      "epoch": 0.8675279469844033,
      "grad_norm": 3.692084789276123,
      "learning_rate": 2.8336902212705215e-05,
      "loss": 2.4833,
      "step": 2430
    },
    {
      "epoch": 0.8710980208403062,
      "grad_norm": 2.9507524967193604,
      "learning_rate": 2.824768022840828e-05,
      "loss": 2.3778,
      "step": 2440
    },
    {
      "epoch": 0.874668094696209,
      "grad_norm": 3.563838005065918,
      "learning_rate": 2.815845824411135e-05,
      "loss": 2.5713,
      "step": 2450
    },
    {
      "epoch": 0.8782381685521119,
      "grad_norm": 2.877849817276001,
      "learning_rate": 2.8069236259814418e-05,
      "loss": 2.4104,
      "step": 2460
    },
    {
      "epoch": 0.8818082424080148,
      "grad_norm": 3.904176950454712,
      "learning_rate": 2.798001427551749e-05,
      "loss": 2.5451,
      "step": 2470
    },
    {
      "epoch": 0.8853783162639177,
      "grad_norm": 2.790254831314087,
      "learning_rate": 2.7890792291220558e-05,
      "loss": 2.5704,
      "step": 2480
    },
    {
      "epoch": 0.8889483901198206,
      "grad_norm": 3.403608798980713,
      "learning_rate": 2.780157030692363e-05,
      "loss": 2.4927,
      "step": 2490
    },
    {
      "epoch": 0.8925184639757235,
      "grad_norm": 3.4232661724090576,
      "learning_rate": 2.7712348322626698e-05,
      "loss": 2.3491,
      "step": 2500
    },
    {
      "epoch": 0.8960885378316263,
      "grad_norm": 3.1123719215393066,
      "learning_rate": 2.7623126338329765e-05,
      "loss": 2.547,
      "step": 2510
    },
    {
      "epoch": 0.8996586116875293,
      "grad_norm": 3.0409297943115234,
      "learning_rate": 2.7533904354032835e-05,
      "loss": 2.6892,
      "step": 2520
    },
    {
      "epoch": 0.9032286855434322,
      "grad_norm": 4.215300559997559,
      "learning_rate": 2.74446823697359e-05,
      "loss": 2.5327,
      "step": 2530
    },
    {
      "epoch": 0.9067987593993351,
      "grad_norm": 3.1233365535736084,
      "learning_rate": 2.7355460385438975e-05,
      "loss": 2.4526,
      "step": 2540
    },
    {
      "epoch": 0.910368833255238,
      "grad_norm": 3.759218692779541,
      "learning_rate": 2.726623840114204e-05,
      "loss": 2.6478,
      "step": 2550
    },
    {
      "epoch": 0.9139389071111409,
      "grad_norm": 3.1220004558563232,
      "learning_rate": 2.7177016416845115e-05,
      "loss": 2.5478,
      "step": 2560
    },
    {
      "epoch": 0.9175089809670438,
      "grad_norm": 3.4189376831054688,
      "learning_rate": 2.708779443254818e-05,
      "loss": 2.4753,
      "step": 2570
    },
    {
      "epoch": 0.9210790548229466,
      "grad_norm": 3.595209836959839,
      "learning_rate": 2.699857244825125e-05,
      "loss": 2.7415,
      "step": 2580
    },
    {
      "epoch": 0.9246491286788495,
      "grad_norm": 2.8045952320098877,
      "learning_rate": 2.6909350463954318e-05,
      "loss": 2.315,
      "step": 2590
    },
    {
      "epoch": 0.9282192025347524,
      "grad_norm": 3.6466944217681885,
      "learning_rate": 2.682012847965739e-05,
      "loss": 2.5102,
      "step": 2600
    },
    {
      "epoch": 0.9317892763906553,
      "grad_norm": 3.8240792751312256,
      "learning_rate": 2.6730906495360458e-05,
      "loss": 2.4755,
      "step": 2610
    },
    {
      "epoch": 0.9353593502465583,
      "grad_norm": 3.432990074157715,
      "learning_rate": 2.6641684511063525e-05,
      "loss": 2.5177,
      "step": 2620
    },
    {
      "epoch": 0.9389294241024612,
      "grad_norm": 4.392916202545166,
      "learning_rate": 2.6552462526766598e-05,
      "loss": 2.5528,
      "step": 2630
    },
    {
      "epoch": 0.942499497958364,
      "grad_norm": 3.8485188484191895,
      "learning_rate": 2.6463240542469665e-05,
      "loss": 2.4854,
      "step": 2640
    },
    {
      "epoch": 0.9460695718142669,
      "grad_norm": 3.649732828140259,
      "learning_rate": 2.6374018558172735e-05,
      "loss": 2.6119,
      "step": 2650
    },
    {
      "epoch": 0.9496396456701698,
      "grad_norm": 3.4265730381011963,
      "learning_rate": 2.62847965738758e-05,
      "loss": 2.5804,
      "step": 2660
    },
    {
      "epoch": 0.9532097195260727,
      "grad_norm": 3.157829999923706,
      "learning_rate": 2.6195574589578875e-05,
      "loss": 2.608,
      "step": 2670
    },
    {
      "epoch": 0.9567797933819756,
      "grad_norm": 3.0505802631378174,
      "learning_rate": 2.610635260528194e-05,
      "loss": 2.6131,
      "step": 2680
    },
    {
      "epoch": 0.9603498672378785,
      "grad_norm": 4.128349781036377,
      "learning_rate": 2.6017130620985015e-05,
      "loss": 2.4171,
      "step": 2690
    },
    {
      "epoch": 0.9639199410937814,
      "grad_norm": 3.199493646621704,
      "learning_rate": 2.592790863668808e-05,
      "loss": 2.587,
      "step": 2700
    },
    {
      "epoch": 0.9674900149496842,
      "grad_norm": 2.8254573345184326,
      "learning_rate": 2.5838686652391155e-05,
      "loss": 2.548,
      "step": 2710
    },
    {
      "epoch": 0.9710600888055871,
      "grad_norm": 3.5668187141418457,
      "learning_rate": 2.574946466809422e-05,
      "loss": 2.7157,
      "step": 2720
    },
    {
      "epoch": 0.97463016266149,
      "grad_norm": 1.8703421354293823,
      "learning_rate": 2.5660242683797288e-05,
      "loss": 2.3831,
      "step": 2730
    },
    {
      "epoch": 0.978200236517393,
      "grad_norm": 3.7826907634735107,
      "learning_rate": 2.5571020699500358e-05,
      "loss": 2.5468,
      "step": 2740
    },
    {
      "epoch": 0.9817703103732959,
      "grad_norm": 3.782118558883667,
      "learning_rate": 2.5481798715203425e-05,
      "loss": 2.5593,
      "step": 2750
    },
    {
      "epoch": 0.9853403842291988,
      "grad_norm": 2.6842267513275146,
      "learning_rate": 2.5392576730906498e-05,
      "loss": 2.6233,
      "step": 2760
    },
    {
      "epoch": 0.9889104580851016,
      "grad_norm": 3.89139723777771,
      "learning_rate": 2.5303354746609565e-05,
      "loss": 2.3521,
      "step": 2770
    },
    {
      "epoch": 0.9924805319410045,
      "grad_norm": 3.251831531524658,
      "learning_rate": 2.5214132762312638e-05,
      "loss": 2.5961,
      "step": 2780
    },
    {
      "epoch": 0.9960506057969074,
      "grad_norm": 2.612891674041748,
      "learning_rate": 2.5124910778015705e-05,
      "loss": 2.4638,
      "step": 2790
    },
    {
      "epoch": 0.9996206796528103,
      "grad_norm": 3.5791077613830566,
      "learning_rate": 2.5035688793718775e-05,
      "loss": 2.5693,
      "step": 2800
    },
    {
      "epoch": 1.0028560590847224,
      "grad_norm": 2.953254461288452,
      "learning_rate": 2.4955389007851536e-05,
      "loss": 2.3104,
      "step": 2810
    },
    {
      "epoch": 1.0064261329406252,
      "grad_norm": 3.1502578258514404,
      "learning_rate": 2.4866167023554603e-05,
      "loss": 2.4082,
      "step": 2820
    },
    {
      "epoch": 1.009996206796528,
      "grad_norm": 2.644451379776001,
      "learning_rate": 2.4776945039257673e-05,
      "loss": 2.2979,
      "step": 2830
    },
    {
      "epoch": 1.013566280652431,
      "grad_norm": 4.506655693054199,
      "learning_rate": 2.4687723054960743e-05,
      "loss": 2.5762,
      "step": 2840
    },
    {
      "epoch": 1.0171363545083338,
      "grad_norm": 3.008094072341919,
      "learning_rate": 2.4598501070663813e-05,
      "loss": 2.3936,
      "step": 2850
    },
    {
      "epoch": 1.0207064283642369,
      "grad_norm": 3.4421534538269043,
      "learning_rate": 2.4509279086366883e-05,
      "loss": 2.4623,
      "step": 2860
    },
    {
      "epoch": 1.0242765022201397,
      "grad_norm": 3.4168381690979004,
      "learning_rate": 2.4420057102069953e-05,
      "loss": 2.4125,
      "step": 2870
    },
    {
      "epoch": 1.0278465760760427,
      "grad_norm": 4.239203929901123,
      "learning_rate": 2.433083511777302e-05,
      "loss": 2.2578,
      "step": 2880
    },
    {
      "epoch": 1.0314166499319455,
      "grad_norm": 3.40299654006958,
      "learning_rate": 2.424161313347609e-05,
      "loss": 2.567,
      "step": 2890
    },
    {
      "epoch": 1.0349867237878483,
      "grad_norm": 3.9125704765319824,
      "learning_rate": 2.415239114917916e-05,
      "loss": 2.4592,
      "step": 2900
    },
    {
      "epoch": 1.0385567976437513,
      "grad_norm": 3.4889445304870605,
      "learning_rate": 2.406316916488223e-05,
      "loss": 2.5213,
      "step": 2910
    },
    {
      "epoch": 1.042126871499654,
      "grad_norm": 3.6200811862945557,
      "learning_rate": 2.3973947180585296e-05,
      "loss": 2.57,
      "step": 2920
    },
    {
      "epoch": 1.0456969453555571,
      "grad_norm": 3.167017936706543,
      "learning_rate": 2.3884725196288366e-05,
      "loss": 2.599,
      "step": 2930
    },
    {
      "epoch": 1.04926701921146,
      "grad_norm": 4.6918206214904785,
      "learning_rate": 2.3795503211991436e-05,
      "loss": 2.4322,
      "step": 2940
    },
    {
      "epoch": 1.0528370930673627,
      "grad_norm": 3.304213285446167,
      "learning_rate": 2.3706281227694506e-05,
      "loss": 2.333,
      "step": 2950
    },
    {
      "epoch": 1.0564071669232658,
      "grad_norm": 3.0455548763275146,
      "learning_rate": 2.3617059243397573e-05,
      "loss": 2.4707,
      "step": 2960
    },
    {
      "epoch": 1.0599772407791686,
      "grad_norm": 5.012803077697754,
      "learning_rate": 2.3527837259100643e-05,
      "loss": 2.5673,
      "step": 2970
    },
    {
      "epoch": 1.0635473146350716,
      "grad_norm": 3.9579105377197266,
      "learning_rate": 2.3438615274803713e-05,
      "loss": 2.4356,
      "step": 2980
    },
    {
      "epoch": 1.0671173884909744,
      "grad_norm": 3.0516128540039062,
      "learning_rate": 2.3349393290506783e-05,
      "loss": 2.3887,
      "step": 2990
    },
    {
      "epoch": 1.0706874623468774,
      "grad_norm": 3.854980230331421,
      "learning_rate": 2.3260171306209853e-05,
      "loss": 2.6053,
      "step": 3000
    },
    {
      "epoch": 1.0742575362027802,
      "grad_norm": 3.545644998550415,
      "learning_rate": 2.3170949321912923e-05,
      "loss": 2.4352,
      "step": 3010
    },
    {
      "epoch": 1.077827610058683,
      "grad_norm": 4.2172417640686035,
      "learning_rate": 2.308172733761599e-05,
      "loss": 2.4507,
      "step": 3020
    },
    {
      "epoch": 1.081397683914586,
      "grad_norm": 2.889023542404175,
      "learning_rate": 2.2992505353319056e-05,
      "loss": 2.5142,
      "step": 3030
    },
    {
      "epoch": 1.0849677577704888,
      "grad_norm": 4.209799766540527,
      "learning_rate": 2.2903283369022126e-05,
      "loss": 2.5281,
      "step": 3040
    },
    {
      "epoch": 1.0885378316263918,
      "grad_norm": 3.43090558052063,
      "learning_rate": 2.2814061384725196e-05,
      "loss": 2.5478,
      "step": 3050
    },
    {
      "epoch": 1.0921079054822946,
      "grad_norm": 3.109950304031372,
      "learning_rate": 2.2724839400428266e-05,
      "loss": 2.5039,
      "step": 3060
    },
    {
      "epoch": 1.0956779793381977,
      "grad_norm": 3.926799774169922,
      "learning_rate": 2.2635617416131336e-05,
      "loss": 2.5378,
      "step": 3070
    },
    {
      "epoch": 1.0992480531941005,
      "grad_norm": 3.1169910430908203,
      "learning_rate": 2.2546395431834406e-05,
      "loss": 2.5687,
      "step": 3080
    },
    {
      "epoch": 1.1028181270500033,
      "grad_norm": 3.795773506164551,
      "learning_rate": 2.2457173447537476e-05,
      "loss": 2.4341,
      "step": 3090
    },
    {
      "epoch": 1.1063882009059063,
      "grad_norm": 2.642240285873413,
      "learning_rate": 2.2367951463240543e-05,
      "loss": 2.6035,
      "step": 3100
    },
    {
      "epoch": 1.109958274761809,
      "grad_norm": 2.8071649074554443,
      "learning_rate": 2.2278729478943613e-05,
      "loss": 2.413,
      "step": 3110
    },
    {
      "epoch": 1.113528348617712,
      "grad_norm": 4.50564432144165,
      "learning_rate": 2.2189507494646683e-05,
      "loss": 2.5392,
      "step": 3120
    },
    {
      "epoch": 1.117098422473615,
      "grad_norm": 3.8215861320495605,
      "learning_rate": 2.2100285510349753e-05,
      "loss": 2.4419,
      "step": 3130
    },
    {
      "epoch": 1.1206684963295177,
      "grad_norm": 4.369407653808594,
      "learning_rate": 2.201106352605282e-05,
      "loss": 2.4994,
      "step": 3140
    },
    {
      "epoch": 1.1242385701854207,
      "grad_norm": 3.5129284858703613,
      "learning_rate": 2.192184154175589e-05,
      "loss": 2.4077,
      "step": 3150
    },
    {
      "epoch": 1.1278086440413235,
      "grad_norm": 3.6634538173675537,
      "learning_rate": 2.183261955745896e-05,
      "loss": 2.6078,
      "step": 3160
    },
    {
      "epoch": 1.1313787178972265,
      "grad_norm": 4.627994537353516,
      "learning_rate": 2.1743397573162026e-05,
      "loss": 2.5692,
      "step": 3170
    },
    {
      "epoch": 1.1349487917531293,
      "grad_norm": 3.2723491191864014,
      "learning_rate": 2.1654175588865096e-05,
      "loss": 2.3972,
      "step": 3180
    },
    {
      "epoch": 1.1385188656090324,
      "grad_norm": 3.3960282802581787,
      "learning_rate": 2.1564953604568166e-05,
      "loss": 2.3709,
      "step": 3190
    },
    {
      "epoch": 1.1420889394649352,
      "grad_norm": 2.801199197769165,
      "learning_rate": 2.1475731620271236e-05,
      "loss": 2.3401,
      "step": 3200
    },
    {
      "epoch": 1.145659013320838,
      "grad_norm": 3.6094534397125244,
      "learning_rate": 2.1386509635974306e-05,
      "loss": 2.456,
      "step": 3210
    },
    {
      "epoch": 1.149229087176741,
      "grad_norm": 3.6080517768859863,
      "learning_rate": 2.1297287651677376e-05,
      "loss": 2.3931,
      "step": 3220
    },
    {
      "epoch": 1.1527991610326438,
      "grad_norm": 3.8434696197509766,
      "learning_rate": 2.1208065667380446e-05,
      "loss": 2.4947,
      "step": 3230
    },
    {
      "epoch": 1.1563692348885468,
      "grad_norm": 3.7189903259277344,
      "learning_rate": 2.1118843683083513e-05,
      "loss": 2.5141,
      "step": 3240
    },
    {
      "epoch": 1.1599393087444496,
      "grad_norm": 4.342618942260742,
      "learning_rate": 2.102962169878658e-05,
      "loss": 2.4769,
      "step": 3250
    },
    {
      "epoch": 1.1635093826003526,
      "grad_norm": 3.2281346321105957,
      "learning_rate": 2.094039971448965e-05,
      "loss": 2.4129,
      "step": 3260
    },
    {
      "epoch": 1.1670794564562554,
      "grad_norm": 3.319664239883423,
      "learning_rate": 2.085117773019272e-05,
      "loss": 2.6191,
      "step": 3270
    },
    {
      "epoch": 1.1706495303121582,
      "grad_norm": 4.064975261688232,
      "learning_rate": 2.076195574589579e-05,
      "loss": 2.4504,
      "step": 3280
    },
    {
      "epoch": 1.1742196041680613,
      "grad_norm": 5.439453125,
      "learning_rate": 2.067273376159886e-05,
      "loss": 2.3441,
      "step": 3290
    },
    {
      "epoch": 1.177789678023964,
      "grad_norm": 5.364106178283691,
      "learning_rate": 2.058351177730193e-05,
      "loss": 2.3883,
      "step": 3300
    },
    {
      "epoch": 1.181359751879867,
      "grad_norm": 3.386218786239624,
      "learning_rate": 2.0494289793004996e-05,
      "loss": 2.5701,
      "step": 3310
    },
    {
      "epoch": 1.1849298257357699,
      "grad_norm": 3.881432056427002,
      "learning_rate": 2.0405067808708066e-05,
      "loss": 2.501,
      "step": 3320
    },
    {
      "epoch": 1.188499899591673,
      "grad_norm": 4.259718894958496,
      "learning_rate": 2.0315845824411136e-05,
      "loss": 2.5537,
      "step": 3330
    },
    {
      "epoch": 1.1920699734475757,
      "grad_norm": 3.297645330429077,
      "learning_rate": 2.0226623840114206e-05,
      "loss": 2.5521,
      "step": 3340
    },
    {
      "epoch": 1.1956400473034785,
      "grad_norm": 2.9119420051574707,
      "learning_rate": 2.0137401855817276e-05,
      "loss": 2.448,
      "step": 3350
    },
    {
      "epoch": 1.1992101211593815,
      "grad_norm": 3.735443353652954,
      "learning_rate": 2.0048179871520343e-05,
      "loss": 2.4342,
      "step": 3360
    },
    {
      "epoch": 1.2027801950152843,
      "grad_norm": 3.526536226272583,
      "learning_rate": 1.9958957887223413e-05,
      "loss": 2.4631,
      "step": 3370
    },
    {
      "epoch": 1.2063502688711873,
      "grad_norm": 4.297111511230469,
      "learning_rate": 1.9869735902926483e-05,
      "loss": 2.4818,
      "step": 3380
    },
    {
      "epoch": 1.2099203427270901,
      "grad_norm": 4.925111293792725,
      "learning_rate": 1.978051391862955e-05,
      "loss": 2.4192,
      "step": 3390
    },
    {
      "epoch": 1.2134904165829932,
      "grad_norm": 3.2096498012542725,
      "learning_rate": 1.969129193433262e-05,
      "loss": 2.5411,
      "step": 3400
    },
    {
      "epoch": 1.217060490438896,
      "grad_norm": 3.202059268951416,
      "learning_rate": 1.960206995003569e-05,
      "loss": 2.4881,
      "step": 3410
    },
    {
      "epoch": 1.2206305642947988,
      "grad_norm": 3.93628191947937,
      "learning_rate": 1.951284796573876e-05,
      "loss": 2.5744,
      "step": 3420
    },
    {
      "epoch": 1.2242006381507018,
      "grad_norm": 3.731104850769043,
      "learning_rate": 1.942362598144183e-05,
      "loss": 2.3722,
      "step": 3430
    },
    {
      "epoch": 1.2277707120066046,
      "grad_norm": 3.706547498703003,
      "learning_rate": 1.93344039971449e-05,
      "loss": 2.4403,
      "step": 3440
    },
    {
      "epoch": 1.2313407858625076,
      "grad_norm": 3.2077808380126953,
      "learning_rate": 1.924518201284797e-05,
      "loss": 2.5671,
      "step": 3450
    },
    {
      "epoch": 1.2349108597184104,
      "grad_norm": 3.8425707817077637,
      "learning_rate": 1.9155960028551036e-05,
      "loss": 2.5319,
      "step": 3460
    },
    {
      "epoch": 1.2384809335743134,
      "grad_norm": 3.4357991218566895,
      "learning_rate": 1.9066738044254103e-05,
      "loss": 2.5275,
      "step": 3470
    },
    {
      "epoch": 1.2420510074302162,
      "grad_norm": 3.4621992111206055,
      "learning_rate": 1.8977516059957173e-05,
      "loss": 2.4919,
      "step": 3480
    },
    {
      "epoch": 1.245621081286119,
      "grad_norm": 3.3455991744995117,
      "learning_rate": 1.8888294075660243e-05,
      "loss": 2.4169,
      "step": 3490
    },
    {
      "epoch": 1.249191155142022,
      "grad_norm": 3.27719783782959,
      "learning_rate": 1.8799072091363313e-05,
      "loss": 2.4016,
      "step": 3500
    },
    {
      "epoch": 1.2527612289979249,
      "grad_norm": 3.766925573348999,
      "learning_rate": 1.8709850107066383e-05,
      "loss": 2.2929,
      "step": 3510
    },
    {
      "epoch": 1.2563313028538277,
      "grad_norm": 3.6688625812530518,
      "learning_rate": 1.8620628122769453e-05,
      "loss": 2.5857,
      "step": 3520
    },
    {
      "epoch": 1.2599013767097307,
      "grad_norm": 3.1747984886169434,
      "learning_rate": 1.853140613847252e-05,
      "loss": 2.3568,
      "step": 3530
    },
    {
      "epoch": 1.2634714505656337,
      "grad_norm": 4.729746341705322,
      "learning_rate": 1.844218415417559e-05,
      "loss": 2.5038,
      "step": 3540
    },
    {
      "epoch": 1.2670415244215365,
      "grad_norm": 4.242550373077393,
      "learning_rate": 1.835296216987866e-05,
      "loss": 2.3357,
      "step": 3550
    },
    {
      "epoch": 1.2706115982774393,
      "grad_norm": 3.5140724182128906,
      "learning_rate": 1.826374018558173e-05,
      "loss": 2.3583,
      "step": 3560
    },
    {
      "epoch": 1.2741816721333423,
      "grad_norm": 5.253378391265869,
      "learning_rate": 1.81745182012848e-05,
      "loss": 2.5548,
      "step": 3570
    },
    {
      "epoch": 1.2777517459892451,
      "grad_norm": 3.128213405609131,
      "learning_rate": 1.8085296216987866e-05,
      "loss": 2.4262,
      "step": 3580
    },
    {
      "epoch": 1.281321819845148,
      "grad_norm": 4.006912708282471,
      "learning_rate": 1.7996074232690936e-05,
      "loss": 2.6098,
      "step": 3590
    },
    {
      "epoch": 1.284891893701051,
      "grad_norm": 3.8548314571380615,
      "learning_rate": 1.7906852248394006e-05,
      "loss": 2.4159,
      "step": 3600
    },
    {
      "epoch": 1.288461967556954,
      "grad_norm": 4.81770133972168,
      "learning_rate": 1.7817630264097072e-05,
      "loss": 2.4561,
      "step": 3610
    },
    {
      "epoch": 1.2920320414128568,
      "grad_norm": 3.7732808589935303,
      "learning_rate": 1.7728408279800142e-05,
      "loss": 2.4967,
      "step": 3620
    },
    {
      "epoch": 1.2956021152687596,
      "grad_norm": 2.8605692386627197,
      "learning_rate": 1.7639186295503213e-05,
      "loss": 2.6385,
      "step": 3630
    },
    {
      "epoch": 1.2991721891246626,
      "grad_norm": 3.653517961502075,
      "learning_rate": 1.7549964311206283e-05,
      "loss": 2.4545,
      "step": 3640
    },
    {
      "epoch": 1.3027422629805654,
      "grad_norm": 3.7104837894439697,
      "learning_rate": 1.7460742326909353e-05,
      "loss": 2.3479,
      "step": 3650
    },
    {
      "epoch": 1.3063123368364682,
      "grad_norm": 3.830782413482666,
      "learning_rate": 1.7371520342612423e-05,
      "loss": 2.5027,
      "step": 3660
    },
    {
      "epoch": 1.3098824106923712,
      "grad_norm": 3.230318307876587,
      "learning_rate": 1.728229835831549e-05,
      "loss": 2.3794,
      "step": 3670
    },
    {
      "epoch": 1.313452484548274,
      "grad_norm": 3.700016498565674,
      "learning_rate": 1.719307637401856e-05,
      "loss": 2.4186,
      "step": 3680
    },
    {
      "epoch": 1.317022558404177,
      "grad_norm": 3.8425114154815674,
      "learning_rate": 1.7103854389721626e-05,
      "loss": 2.4297,
      "step": 3690
    },
    {
      "epoch": 1.3205926322600798,
      "grad_norm": 5.092566013336182,
      "learning_rate": 1.7014632405424696e-05,
      "loss": 2.2917,
      "step": 3700
    },
    {
      "epoch": 1.3241627061159829,
      "grad_norm": 3.45686674118042,
      "learning_rate": 1.6925410421127766e-05,
      "loss": 2.4832,
      "step": 3710
    },
    {
      "epoch": 1.3277327799718857,
      "grad_norm": 5.492971420288086,
      "learning_rate": 1.6836188436830836e-05,
      "loss": 2.255,
      "step": 3720
    },
    {
      "epoch": 1.3313028538277885,
      "grad_norm": 4.659407138824463,
      "learning_rate": 1.6746966452533906e-05,
      "loss": 2.5989,
      "step": 3730
    },
    {
      "epoch": 1.3348729276836915,
      "grad_norm": 3.2591938972473145,
      "learning_rate": 1.6657744468236976e-05,
      "loss": 2.4317,
      "step": 3740
    },
    {
      "epoch": 1.3384430015395943,
      "grad_norm": 3.595890998840332,
      "learning_rate": 1.6568522483940042e-05,
      "loss": 2.2815,
      "step": 3750
    },
    {
      "epoch": 1.3420130753954973,
      "grad_norm": 4.103309154510498,
      "learning_rate": 1.6479300499643112e-05,
      "loss": 2.3444,
      "step": 3760
    },
    {
      "epoch": 1.3455831492514,
      "grad_norm": 3.07283353805542,
      "learning_rate": 1.6390078515346182e-05,
      "loss": 2.468,
      "step": 3770
    },
    {
      "epoch": 1.3491532231073031,
      "grad_norm": 4.009388446807861,
      "learning_rate": 1.6300856531049252e-05,
      "loss": 2.4749,
      "step": 3780
    },
    {
      "epoch": 1.352723296963206,
      "grad_norm": 5.529467582702637,
      "learning_rate": 1.6211634546752322e-05,
      "loss": 2.5439,
      "step": 3790
    },
    {
      "epoch": 1.3562933708191087,
      "grad_norm": 3.8818471431732178,
      "learning_rate": 1.612241256245539e-05,
      "loss": 2.3179,
      "step": 3800
    },
    {
      "epoch": 1.3598634446750117,
      "grad_norm": 3.6723248958587646,
      "learning_rate": 1.603319057815846e-05,
      "loss": 2.5332,
      "step": 3810
    },
    {
      "epoch": 1.3634335185309145,
      "grad_norm": 3.888360023498535,
      "learning_rate": 1.5943968593861526e-05,
      "loss": 2.4548,
      "step": 3820
    },
    {
      "epoch": 1.3670035923868176,
      "grad_norm": 4.061384677886963,
      "learning_rate": 1.5854746609564596e-05,
      "loss": 2.4974,
      "step": 3830
    },
    {
      "epoch": 1.3705736662427204,
      "grad_norm": 2.3812010288238525,
      "learning_rate": 1.5765524625267666e-05,
      "loss": 2.4207,
      "step": 3840
    },
    {
      "epoch": 1.3741437400986234,
      "grad_norm": 3.970087766647339,
      "learning_rate": 1.5676302640970736e-05,
      "loss": 2.6048,
      "step": 3850
    },
    {
      "epoch": 1.3777138139545262,
      "grad_norm": 3.51749587059021,
      "learning_rate": 1.5587080656673806e-05,
      "loss": 2.4603,
      "step": 3860
    },
    {
      "epoch": 1.381283887810429,
      "grad_norm": 3.8897745609283447,
      "learning_rate": 1.5497858672376876e-05,
      "loss": 2.4836,
      "step": 3870
    },
    {
      "epoch": 1.384853961666332,
      "grad_norm": 3.4729361534118652,
      "learning_rate": 1.5408636688079946e-05,
      "loss": 2.448,
      "step": 3880
    },
    {
      "epoch": 1.3884240355222348,
      "grad_norm": 5.6703691482543945,
      "learning_rate": 1.5319414703783012e-05,
      "loss": 2.3626,
      "step": 3890
    },
    {
      "epoch": 1.3919941093781378,
      "grad_norm": 3.651689052581787,
      "learning_rate": 1.5230192719486084e-05,
      "loss": 2.5051,
      "step": 3900
    },
    {
      "epoch": 1.3955641832340406,
      "grad_norm": 3.7923481464385986,
      "learning_rate": 1.514097073518915e-05,
      "loss": 2.4286,
      "step": 3910
    },
    {
      "epoch": 1.3991342570899437,
      "grad_norm": 3.4599170684814453,
      "learning_rate": 1.5051748750892219e-05,
      "loss": 2.5535,
      "step": 3920
    },
    {
      "epoch": 1.4027043309458465,
      "grad_norm": 3.7359261512756348,
      "learning_rate": 1.4962526766595289e-05,
      "loss": 2.3136,
      "step": 3930
    },
    {
      "epoch": 1.4062744048017493,
      "grad_norm": 3.1897103786468506,
      "learning_rate": 1.4873304782298359e-05,
      "loss": 2.6154,
      "step": 3940
    },
    {
      "epoch": 1.4098444786576523,
      "grad_norm": 3.7598700523376465,
      "learning_rate": 1.4784082798001427e-05,
      "loss": 2.6579,
      "step": 3950
    },
    {
      "epoch": 1.413414552513555,
      "grad_norm": 3.9269566535949707,
      "learning_rate": 1.4694860813704497e-05,
      "loss": 2.4982,
      "step": 3960
    },
    {
      "epoch": 1.416984626369458,
      "grad_norm": 3.2080864906311035,
      "learning_rate": 1.4605638829407567e-05,
      "loss": 2.6423,
      "step": 3970
    },
    {
      "epoch": 1.420554700225361,
      "grad_norm": 3.9937877655029297,
      "learning_rate": 1.4516416845110636e-05,
      "loss": 2.4499,
      "step": 3980
    },
    {
      "epoch": 1.424124774081264,
      "grad_norm": 3.714174747467041,
      "learning_rate": 1.4427194860813706e-05,
      "loss": 2.473,
      "step": 3990
    },
    {
      "epoch": 1.4276948479371667,
      "grad_norm": 3.430488348007202,
      "learning_rate": 1.4337972876516776e-05,
      "loss": 2.5091,
      "step": 4000
    },
    {
      "epoch": 1.4312649217930695,
      "grad_norm": 3.7093393802642822,
      "learning_rate": 1.4248750892219844e-05,
      "loss": 2.3017,
      "step": 4010
    },
    {
      "epoch": 1.4348349956489725,
      "grad_norm": 3.369459390640259,
      "learning_rate": 1.4159528907922912e-05,
      "loss": 2.3226,
      "step": 4020
    },
    {
      "epoch": 1.4384050695048753,
      "grad_norm": 3.7917959690093994,
      "learning_rate": 1.407030692362598e-05,
      "loss": 2.5041,
      "step": 4030
    },
    {
      "epoch": 1.4419751433607781,
      "grad_norm": 3.886899948120117,
      "learning_rate": 1.398108493932905e-05,
      "loss": 2.3289,
      "step": 4040
    },
    {
      "epoch": 1.4455452172166812,
      "grad_norm": 3.627056360244751,
      "learning_rate": 1.389186295503212e-05,
      "loss": 2.47,
      "step": 4050
    },
    {
      "epoch": 1.4491152910725842,
      "grad_norm": 3.9948859214782715,
      "learning_rate": 1.3802640970735189e-05,
      "loss": 2.4209,
      "step": 4060
    },
    {
      "epoch": 1.452685364928487,
      "grad_norm": 3.5933163166046143,
      "learning_rate": 1.3713418986438259e-05,
      "loss": 2.3677,
      "step": 4070
    },
    {
      "epoch": 1.4562554387843898,
      "grad_norm": 3.844318389892578,
      "learning_rate": 1.3624197002141329e-05,
      "loss": 2.4517,
      "step": 4080
    },
    {
      "epoch": 1.4598255126402928,
      "grad_norm": 4.4778313636779785,
      "learning_rate": 1.3534975017844397e-05,
      "loss": 2.4784,
      "step": 4090
    },
    {
      "epoch": 1.4633955864961956,
      "grad_norm": 3.314497470855713,
      "learning_rate": 1.3445753033547467e-05,
      "loss": 2.4036,
      "step": 4100
    },
    {
      "epoch": 1.4669656603520984,
      "grad_norm": 4.35644006729126,
      "learning_rate": 1.3356531049250537e-05,
      "loss": 2.3999,
      "step": 4110
    },
    {
      "epoch": 1.4705357342080014,
      "grad_norm": 3.607351779937744,
      "learning_rate": 1.3267309064953606e-05,
      "loss": 2.473,
      "step": 4120
    },
    {
      "epoch": 1.4741058080639045,
      "grad_norm": 4.234920978546143,
      "learning_rate": 1.3178087080656672e-05,
      "loss": 2.3218,
      "step": 4130
    },
    {
      "epoch": 1.4776758819198073,
      "grad_norm": 3.6296091079711914,
      "learning_rate": 1.3088865096359742e-05,
      "loss": 2.4187,
      "step": 4140
    },
    {
      "epoch": 1.48124595577571,
      "grad_norm": 3.917867422103882,
      "learning_rate": 1.2999643112062812e-05,
      "loss": 2.4716,
      "step": 4150
    },
    {
      "epoch": 1.484816029631613,
      "grad_norm": 3.228609085083008,
      "learning_rate": 1.2910421127765882e-05,
      "loss": 2.436,
      "step": 4160
    },
    {
      "epoch": 1.4883861034875159,
      "grad_norm": 4.389735221862793,
      "learning_rate": 1.282119914346895e-05,
      "loss": 2.4796,
      "step": 4170
    },
    {
      "epoch": 1.4919561773434187,
      "grad_norm": 3.961900472640991,
      "learning_rate": 1.273197715917202e-05,
      "loss": 2.611,
      "step": 4180
    },
    {
      "epoch": 1.4955262511993217,
      "grad_norm": 3.9580492973327637,
      "learning_rate": 1.264275517487509e-05,
      "loss": 2.2878,
      "step": 4190
    },
    {
      "epoch": 1.4990963250552245,
      "grad_norm": 3.199282169342041,
      "learning_rate": 1.2553533190578159e-05,
      "loss": 2.4738,
      "step": 4200
    },
    {
      "epoch": 1.5026663989111275,
      "grad_norm": 3.394921064376831,
      "learning_rate": 1.2464311206281229e-05,
      "loss": 2.3578,
      "step": 4210
    },
    {
      "epoch": 1.5062364727670303,
      "grad_norm": 3.074375867843628,
      "learning_rate": 1.2375089221984297e-05,
      "loss": 2.3499,
      "step": 4220
    },
    {
      "epoch": 1.5098065466229333,
      "grad_norm": 3.775493621826172,
      "learning_rate": 1.2285867237687367e-05,
      "loss": 2.4622,
      "step": 4230
    },
    {
      "epoch": 1.5133766204788361,
      "grad_norm": 4.119907855987549,
      "learning_rate": 1.2196645253390436e-05,
      "loss": 2.4422,
      "step": 4240
    },
    {
      "epoch": 1.516946694334739,
      "grad_norm": 3.3789119720458984,
      "learning_rate": 1.2107423269093506e-05,
      "loss": 2.3553,
      "step": 4250
    },
    {
      "epoch": 1.520516768190642,
      "grad_norm": 4.212026119232178,
      "learning_rate": 1.2018201284796576e-05,
      "loss": 2.3927,
      "step": 4260
    },
    {
      "epoch": 1.524086842046545,
      "grad_norm": 3.6889474391937256,
      "learning_rate": 1.1928979300499644e-05,
      "loss": 2.3021,
      "step": 4270
    },
    {
      "epoch": 1.5276569159024478,
      "grad_norm": 4.331241607666016,
      "learning_rate": 1.1839757316202712e-05,
      "loss": 2.3961,
      "step": 4280
    },
    {
      "epoch": 1.5312269897583506,
      "grad_norm": 4.286965847015381,
      "learning_rate": 1.1750535331905782e-05,
      "loss": 2.5325,
      "step": 4290
    },
    {
      "epoch": 1.5347970636142536,
      "grad_norm": 3.7933521270751953,
      "learning_rate": 1.1661313347608852e-05,
      "loss": 2.332,
      "step": 4300
    },
    {
      "epoch": 1.5383671374701564,
      "grad_norm": 3.8488738536834717,
      "learning_rate": 1.157209136331192e-05,
      "loss": 2.254,
      "step": 4310
    },
    {
      "epoch": 1.5419372113260592,
      "grad_norm": 3.3081209659576416,
      "learning_rate": 1.148286937901499e-05,
      "loss": 2.3272,
      "step": 4320
    },
    {
      "epoch": 1.5455072851819622,
      "grad_norm": 2.3623673915863037,
      "learning_rate": 1.1393647394718059e-05,
      "loss": 2.3092,
      "step": 4330
    },
    {
      "epoch": 1.5490773590378653,
      "grad_norm": 3.4768309593200684,
      "learning_rate": 1.1304425410421129e-05,
      "loss": 2.3843,
      "step": 4340
    },
    {
      "epoch": 1.5526474328937678,
      "grad_norm": 4.5293755531311035,
      "learning_rate": 1.1215203426124197e-05,
      "loss": 2.5101,
      "step": 4350
    },
    {
      "epoch": 1.5562175067496709,
      "grad_norm": 4.2419610023498535,
      "learning_rate": 1.1125981441827267e-05,
      "loss": 2.5039,
      "step": 4360
    },
    {
      "epoch": 1.5597875806055739,
      "grad_norm": 4.437801361083984,
      "learning_rate": 1.1036759457530337e-05,
      "loss": 2.4703,
      "step": 4370
    },
    {
      "epoch": 1.5633576544614767,
      "grad_norm": 4.3072028160095215,
      "learning_rate": 1.0947537473233404e-05,
      "loss": 2.4309,
      "step": 4380
    },
    {
      "epoch": 1.5669277283173795,
      "grad_norm": 3.6231558322906494,
      "learning_rate": 1.0858315488936474e-05,
      "loss": 2.4183,
      "step": 4390
    },
    {
      "epoch": 1.5704978021732825,
      "grad_norm": 3.6076834201812744,
      "learning_rate": 1.0769093504639544e-05,
      "loss": 2.3962,
      "step": 4400
    },
    {
      "epoch": 1.5740678760291855,
      "grad_norm": 4.360884189605713,
      "learning_rate": 1.0679871520342614e-05,
      "loss": 2.4378,
      "step": 4410
    },
    {
      "epoch": 1.577637949885088,
      "grad_norm": 3.483644485473633,
      "learning_rate": 1.0590649536045682e-05,
      "loss": 2.3174,
      "step": 4420
    },
    {
      "epoch": 1.5812080237409911,
      "grad_norm": 4.645442485809326,
      "learning_rate": 1.0501427551748752e-05,
      "loss": 2.5111,
      "step": 4430
    },
    {
      "epoch": 1.5847780975968941,
      "grad_norm": 3.525163412094116,
      "learning_rate": 1.041220556745182e-05,
      "loss": 2.4429,
      "step": 4440
    },
    {
      "epoch": 1.588348171452797,
      "grad_norm": 3.626473903656006,
      "learning_rate": 1.0322983583154889e-05,
      "loss": 2.4428,
      "step": 4450
    },
    {
      "epoch": 1.5919182453086997,
      "grad_norm": 3.2441213130950928,
      "learning_rate": 1.0233761598857959e-05,
      "loss": 2.4502,
      "step": 4460
    },
    {
      "epoch": 1.5954883191646028,
      "grad_norm": 3.5859060287475586,
      "learning_rate": 1.0144539614561029e-05,
      "loss": 2.6327,
      "step": 4470
    },
    {
      "epoch": 1.5990583930205056,
      "grad_norm": 4.145968914031982,
      "learning_rate": 1.0055317630264099e-05,
      "loss": 2.4242,
      "step": 4480
    },
    {
      "epoch": 1.6026284668764084,
      "grad_norm": 5.47335958480835,
      "learning_rate": 9.966095645967165e-06,
      "loss": 2.2599,
      "step": 4490
    },
    {
      "epoch": 1.6061985407323114,
      "grad_norm": 3.5645487308502197,
      "learning_rate": 9.876873661670235e-06,
      "loss": 2.3947,
      "step": 4500
    },
    {
      "epoch": 1.6097686145882144,
      "grad_norm": 3.616346597671509,
      "learning_rate": 9.787651677373305e-06,
      "loss": 2.3585,
      "step": 4510
    },
    {
      "epoch": 1.6133386884441172,
      "grad_norm": 3.9968783855438232,
      "learning_rate": 9.698429693076374e-06,
      "loss": 2.2899,
      "step": 4520
    },
    {
      "epoch": 1.61690876230002,
      "grad_norm": 5.433516502380371,
      "learning_rate": 9.609207708779444e-06,
      "loss": 2.5401,
      "step": 4530
    },
    {
      "epoch": 1.620478836155923,
      "grad_norm": 2.9177725315093994,
      "learning_rate": 9.519985724482514e-06,
      "loss": 2.4202,
      "step": 4540
    },
    {
      "epoch": 1.6240489100118258,
      "grad_norm": 3.431448459625244,
      "learning_rate": 9.430763740185582e-06,
      "loss": 2.5315,
      "step": 4550
    },
    {
      "epoch": 1.6276189838677286,
      "grad_norm": 3.8646445274353027,
      "learning_rate": 9.34154175588865e-06,
      "loss": 2.4095,
      "step": 4560
    },
    {
      "epoch": 1.6311890577236317,
      "grad_norm": 5.92616605758667,
      "learning_rate": 9.25231977159172e-06,
      "loss": 2.4868,
      "step": 4570
    },
    {
      "epoch": 1.6347591315795347,
      "grad_norm": 3.5453314781188965,
      "learning_rate": 9.16309778729479e-06,
      "loss": 2.2682,
      "step": 4580
    },
    {
      "epoch": 1.6383292054354375,
      "grad_norm": 4.392436981201172,
      "learning_rate": 9.073875802997859e-06,
      "loss": 2.3164,
      "step": 4590
    },
    {
      "epoch": 1.6418992792913403,
      "grad_norm": 3.42264461517334,
      "learning_rate": 8.984653818700927e-06,
      "loss": 2.4317,
      "step": 4600
    },
    {
      "epoch": 1.6454693531472433,
      "grad_norm": 3.6263813972473145,
      "learning_rate": 8.895431834403997e-06,
      "loss": 2.547,
      "step": 4610
    },
    {
      "epoch": 1.649039427003146,
      "grad_norm": 4.718423366546631,
      "learning_rate": 8.806209850107067e-06,
      "loss": 2.4476,
      "step": 4620
    },
    {
      "epoch": 1.652609500859049,
      "grad_norm": 3.4111568927764893,
      "learning_rate": 8.716987865810135e-06,
      "loss": 2.3037,
      "step": 4630
    },
    {
      "epoch": 1.656179574714952,
      "grad_norm": 4.071990966796875,
      "learning_rate": 8.627765881513205e-06,
      "loss": 2.3592,
      "step": 4640
    },
    {
      "epoch": 1.659749648570855,
      "grad_norm": 3.4774062633514404,
      "learning_rate": 8.538543897216275e-06,
      "loss": 2.482,
      "step": 4650
    },
    {
      "epoch": 1.6633197224267577,
      "grad_norm": 3.9638102054595947,
      "learning_rate": 8.449321912919344e-06,
      "loss": 2.3893,
      "step": 4660
    },
    {
      "epoch": 1.6668897962826605,
      "grad_norm": 7.0258026123046875,
      "learning_rate": 8.360099928622412e-06,
      "loss": 2.4815,
      "step": 4670
    },
    {
      "epoch": 1.6704598701385636,
      "grad_norm": 3.366650104522705,
      "learning_rate": 8.270877944325482e-06,
      "loss": 2.4993,
      "step": 4680
    },
    {
      "epoch": 1.6740299439944664,
      "grad_norm": 4.147978782653809,
      "learning_rate": 8.181655960028552e-06,
      "loss": 2.4005,
      "step": 4690
    },
    {
      "epoch": 1.6776000178503692,
      "grad_norm": 4.391261577606201,
      "learning_rate": 8.09243397573162e-06,
      "loss": 2.4772,
      "step": 4700
    },
    {
      "epoch": 1.6811700917062722,
      "grad_norm": 3.077601194381714,
      "learning_rate": 8.003211991434689e-06,
      "loss": 2.3983,
      "step": 4710
    },
    {
      "epoch": 1.6847401655621752,
      "grad_norm": 3.535592794418335,
      "learning_rate": 7.913990007137759e-06,
      "loss": 2.3596,
      "step": 4720
    },
    {
      "epoch": 1.688310239418078,
      "grad_norm": 3.895205020904541,
      "learning_rate": 7.824768022840829e-06,
      "loss": 2.3376,
      "step": 4730
    },
    {
      "epoch": 1.6918803132739808,
      "grad_norm": 2.9614930152893066,
      "learning_rate": 7.735546038543897e-06,
      "loss": 2.4429,
      "step": 4740
    },
    {
      "epoch": 1.6954503871298838,
      "grad_norm": 3.58000111579895,
      "learning_rate": 7.646324054246967e-06,
      "loss": 2.3233,
      "step": 4750
    },
    {
      "epoch": 1.6990204609857866,
      "grad_norm": 3.8900201320648193,
      "learning_rate": 7.557102069950037e-06,
      "loss": 2.4189,
      "step": 4760
    },
    {
      "epoch": 1.7025905348416894,
      "grad_norm": 3.3871653079986572,
      "learning_rate": 7.4678800856531045e-06,
      "loss": 2.4688,
      "step": 4770
    },
    {
      "epoch": 1.7061606086975925,
      "grad_norm": 4.3925042152404785,
      "learning_rate": 7.3786581013561745e-06,
      "loss": 2.5561,
      "step": 4780
    },
    {
      "epoch": 1.7097306825534955,
      "grad_norm": 3.60677170753479,
      "learning_rate": 7.289436117059244e-06,
      "loss": 2.3876,
      "step": 4790
    },
    {
      "epoch": 1.7133007564093983,
      "grad_norm": 4.014009952545166,
      "learning_rate": 7.200214132762313e-06,
      "loss": 2.4721,
      "step": 4800
    },
    {
      "epoch": 1.716870830265301,
      "grad_norm": 4.606167793273926,
      "learning_rate": 7.110992148465383e-06,
      "loss": 2.4833,
      "step": 4810
    },
    {
      "epoch": 1.720440904121204,
      "grad_norm": 4.394203186035156,
      "learning_rate": 7.021770164168451e-06,
      "loss": 2.4024,
      "step": 4820
    },
    {
      "epoch": 1.724010977977107,
      "grad_norm": 3.95469069480896,
      "learning_rate": 6.93254817987152e-06,
      "loss": 2.2338,
      "step": 4830
    },
    {
      "epoch": 1.7275810518330097,
      "grad_norm": 4.240095615386963,
      "learning_rate": 6.8433261955745895e-06,
      "loss": 2.4918,
      "step": 4840
    },
    {
      "epoch": 1.7311511256889127,
      "grad_norm": 3.1807761192321777,
      "learning_rate": 6.7541042112776595e-06,
      "loss": 2.5089,
      "step": 4850
    },
    {
      "epoch": 1.7347211995448157,
      "grad_norm": 4.186365604400635,
      "learning_rate": 6.664882226980729e-06,
      "loss": 2.4519,
      "step": 4860
    },
    {
      "epoch": 1.7382912734007183,
      "grad_norm": 4.181806564331055,
      "learning_rate": 6.575660242683798e-06,
      "loss": 2.4074,
      "step": 4870
    },
    {
      "epoch": 1.7418613472566213,
      "grad_norm": 4.076844692230225,
      "learning_rate": 6.486438258386866e-06,
      "loss": 2.4767,
      "step": 4880
    },
    {
      "epoch": 1.7454314211125244,
      "grad_norm": 3.3212153911590576,
      "learning_rate": 6.397216274089936e-06,
      "loss": 2.3243,
      "step": 4890
    },
    {
      "epoch": 1.7490014949684272,
      "grad_norm": 3.6174097061157227,
      "learning_rate": 6.307994289793005e-06,
      "loss": 2.4488,
      "step": 4900
    },
    {
      "epoch": 1.75257156882433,
      "grad_norm": 4.748317241668701,
      "learning_rate": 6.2187723054960745e-06,
      "loss": 2.272,
      "step": 4910
    },
    {
      "epoch": 1.756141642680233,
      "grad_norm": 4.333911418914795,
      "learning_rate": 6.129550321199144e-06,
      "loss": 2.3417,
      "step": 4920
    },
    {
      "epoch": 1.759711716536136,
      "grad_norm": 4.413632869720459,
      "learning_rate": 6.040328336902213e-06,
      "loss": 2.5383,
      "step": 4930
    },
    {
      "epoch": 1.7632817903920386,
      "grad_norm": 3.2573864459991455,
      "learning_rate": 5.951106352605283e-06,
      "loss": 2.3816,
      "step": 4940
    },
    {
      "epoch": 1.7668518642479416,
      "grad_norm": 3.3643205165863037,
      "learning_rate": 5.861884368308351e-06,
      "loss": 2.3596,
      "step": 4950
    },
    {
      "epoch": 1.7704219381038446,
      "grad_norm": 3.0364601612091064,
      "learning_rate": 5.772662384011421e-06,
      "loss": 2.3785,
      "step": 4960
    },
    {
      "epoch": 1.7739920119597474,
      "grad_norm": 4.160336971282959,
      "learning_rate": 5.6834403997144894e-06,
      "loss": 2.4305,
      "step": 4970
    },
    {
      "epoch": 1.7775620858156502,
      "grad_norm": 3.1676225662231445,
      "learning_rate": 5.5942184154175594e-06,
      "loss": 2.5009,
      "step": 4980
    },
    {
      "epoch": 1.7811321596715533,
      "grad_norm": 3.800809621810913,
      "learning_rate": 5.504996431120629e-06,
      "loss": 2.3696,
      "step": 4990
    },
    {
      "epoch": 1.784702233527456,
      "grad_norm": 4.364882946014404,
      "learning_rate": 5.415774446823697e-06,
      "loss": 2.3752,
      "step": 5000
    },
    {
      "epoch": 1.7882723073833589,
      "grad_norm": 3.1778082847595215,
      "learning_rate": 5.326552462526767e-06,
      "loss": 2.3173,
      "step": 5010
    },
    {
      "epoch": 1.7918423812392619,
      "grad_norm": 4.438313961029053,
      "learning_rate": 5.237330478229836e-06,
      "loss": 2.4602,
      "step": 5020
    },
    {
      "epoch": 1.795412455095165,
      "grad_norm": 3.900614023208618,
      "learning_rate": 5.148108493932905e-06,
      "loss": 2.3757,
      "step": 5030
    },
    {
      "epoch": 1.7989825289510677,
      "grad_norm": 4.466071128845215,
      "learning_rate": 5.058886509635974e-06,
      "loss": 2.3699,
      "step": 5040
    },
    {
      "epoch": 1.8025526028069705,
      "grad_norm": 6.702785015106201,
      "learning_rate": 4.969664525339044e-06,
      "loss": 2.4399,
      "step": 5050
    },
    {
      "epoch": 1.8061226766628735,
      "grad_norm": 4.654899597167969,
      "learning_rate": 4.880442541042113e-06,
      "loss": 2.5928,
      "step": 5060
    },
    {
      "epoch": 1.8096927505187763,
      "grad_norm": 3.875901460647583,
      "learning_rate": 4.791220556745183e-06,
      "loss": 2.4763,
      "step": 5070
    },
    {
      "epoch": 1.8132628243746791,
      "grad_norm": 4.051814556121826,
      "learning_rate": 4.701998572448252e-06,
      "loss": 2.3547,
      "step": 5080
    },
    {
      "epoch": 1.8168328982305821,
      "grad_norm": 5.24447774887085,
      "learning_rate": 4.61277658815132e-06,
      "loss": 2.4228,
      "step": 5090
    },
    {
      "epoch": 1.8204029720864852,
      "grad_norm": 3.7276995182037354,
      "learning_rate": 4.52355460385439e-06,
      "loss": 2.4287,
      "step": 5100
    },
    {
      "epoch": 1.823973045942388,
      "grad_norm": 3.311311960220337,
      "learning_rate": 4.4343326195574585e-06,
      "loss": 2.2995,
      "step": 5110
    },
    {
      "epoch": 1.8275431197982908,
      "grad_norm": 3.5810890197753906,
      "learning_rate": 4.3451106352605285e-06,
      "loss": 2.5281,
      "step": 5120
    },
    {
      "epoch": 1.8311131936541938,
      "grad_norm": 4.011887073516846,
      "learning_rate": 4.255888650963598e-06,
      "loss": 2.3196,
      "step": 5130
    },
    {
      "epoch": 1.8346832675100966,
      "grad_norm": 3.541322946548462,
      "learning_rate": 4.166666666666667e-06,
      "loss": 2.417,
      "step": 5140
    },
    {
      "epoch": 1.8382533413659994,
      "grad_norm": 6.3039870262146,
      "learning_rate": 4.077444682369736e-06,
      "loss": 2.4342,
      "step": 5150
    },
    {
      "epoch": 1.8418234152219024,
      "grad_norm": 3.7186849117279053,
      "learning_rate": 3.988222698072805e-06,
      "loss": 2.4452,
      "step": 5160
    },
    {
      "epoch": 1.8453934890778054,
      "grad_norm": 4.810699939727783,
      "learning_rate": 3.899000713775874e-06,
      "loss": 2.5106,
      "step": 5170
    },
    {
      "epoch": 1.8489635629337082,
      "grad_norm": 4.267812252044678,
      "learning_rate": 3.809778729478944e-06,
      "loss": 2.3649,
      "step": 5180
    },
    {
      "epoch": 1.852533636789611,
      "grad_norm": 4.023059368133545,
      "learning_rate": 3.7205567451820135e-06,
      "loss": 2.3363,
      "step": 5190
    },
    {
      "epoch": 1.856103710645514,
      "grad_norm": 4.272152423858643,
      "learning_rate": 3.6313347608850823e-06,
      "loss": 2.3146,
      "step": 5200
    },
    {
      "epoch": 1.8596737845014168,
      "grad_norm": 4.1089301109313965,
      "learning_rate": 3.542112776588152e-06,
      "loss": 2.5572,
      "step": 5210
    },
    {
      "epoch": 1.8632438583573196,
      "grad_norm": 3.737591028213501,
      "learning_rate": 3.4528907922912206e-06,
      "loss": 2.4712,
      "step": 5220
    },
    {
      "epoch": 1.8668139322132227,
      "grad_norm": 3.776556968688965,
      "learning_rate": 3.3636688079942897e-06,
      "loss": 2.5261,
      "step": 5230
    },
    {
      "epoch": 1.8703840060691257,
      "grad_norm": 3.3213982582092285,
      "learning_rate": 3.2744468236973593e-06,
      "loss": 2.2866,
      "step": 5240
    },
    {
      "epoch": 1.8739540799250285,
      "grad_norm": 4.501275062561035,
      "learning_rate": 3.185224839400428e-06,
      "loss": 2.5776,
      "step": 5250
    },
    {
      "epoch": 1.8775241537809313,
      "grad_norm": 4.563452243804932,
      "learning_rate": 3.0960028551034976e-06,
      "loss": 2.3828,
      "step": 5260
    },
    {
      "epoch": 1.8810942276368343,
      "grad_norm": 4.946331977844238,
      "learning_rate": 3.006780870806567e-06,
      "loss": 2.4167,
      "step": 5270
    },
    {
      "epoch": 1.8846643014927371,
      "grad_norm": 4.1918134689331055,
      "learning_rate": 2.9175588865096364e-06,
      "loss": 2.5191,
      "step": 5280
    },
    {
      "epoch": 1.88823437534864,
      "grad_norm": 3.4255847930908203,
      "learning_rate": 2.8283369022127056e-06,
      "loss": 2.4409,
      "step": 5290
    },
    {
      "epoch": 1.891804449204543,
      "grad_norm": 3.889915704727173,
      "learning_rate": 2.7391149179157747e-06,
      "loss": 2.38,
      "step": 5300
    },
    {
      "epoch": 1.895374523060446,
      "grad_norm": 3.6254022121429443,
      "learning_rate": 2.649892933618844e-06,
      "loss": 2.4174,
      "step": 5310
    },
    {
      "epoch": 1.8989445969163488,
      "grad_norm": 3.59116792678833,
      "learning_rate": 2.560670949321913e-06,
      "loss": 2.5689,
      "step": 5320
    },
    {
      "epoch": 1.9025146707722516,
      "grad_norm": 3.5882458686828613,
      "learning_rate": 2.471448965024982e-06,
      "loss": 2.3455,
      "step": 5330
    },
    {
      "epoch": 1.9060847446281546,
      "grad_norm": 3.922898054122925,
      "learning_rate": 2.3822269807280514e-06,
      "loss": 2.3947,
      "step": 5340
    },
    {
      "epoch": 1.9096548184840574,
      "grad_norm": 4.161267280578613,
      "learning_rate": 2.2930049964311205e-06,
      "loss": 2.5257,
      "step": 5350
    },
    {
      "epoch": 1.9132248923399602,
      "grad_norm": 2.8744518756866455,
      "learning_rate": 2.20378301213419e-06,
      "loss": 2.4608,
      "step": 5360
    },
    {
      "epoch": 1.9167949661958632,
      "grad_norm": 3.634092330932617,
      "learning_rate": 2.1145610278372593e-06,
      "loss": 2.5989,
      "step": 5370
    },
    {
      "epoch": 1.9203650400517662,
      "grad_norm": 4.193907737731934,
      "learning_rate": 2.0253390435403284e-06,
      "loss": 2.4121,
      "step": 5380
    },
    {
      "epoch": 1.9239351139076688,
      "grad_norm": 3.826251745223999,
      "learning_rate": 1.936117059243398e-06,
      "loss": 2.4743,
      "step": 5390
    },
    {
      "epoch": 1.9275051877635718,
      "grad_norm": 4.273322105407715,
      "learning_rate": 1.846895074946467e-06,
      "loss": 2.2846,
      "step": 5400
    },
    {
      "epoch": 1.9310752616194748,
      "grad_norm": 3.491528034210205,
      "learning_rate": 1.7576730906495361e-06,
      "loss": 2.4362,
      "step": 5410
    },
    {
      "epoch": 1.9346453354753776,
      "grad_norm": 3.2537927627563477,
      "learning_rate": 1.6684511063526053e-06,
      "loss": 2.2657,
      "step": 5420
    },
    {
      "epoch": 1.9382154093312804,
      "grad_norm": 3.364931344985962,
      "learning_rate": 1.5792291220556747e-06,
      "loss": 2.3336,
      "step": 5430
    },
    {
      "epoch": 1.9417854831871835,
      "grad_norm": 3.8835065364837646,
      "learning_rate": 1.4900071377587438e-06,
      "loss": 2.4867,
      "step": 5440
    },
    {
      "epoch": 1.9453555570430865,
      "grad_norm": 3.675105094909668,
      "learning_rate": 1.400785153461813e-06,
      "loss": 2.4899,
      "step": 5450
    },
    {
      "epoch": 1.948925630898989,
      "grad_norm": 4.876898765563965,
      "learning_rate": 1.3115631691648824e-06,
      "loss": 2.5289,
      "step": 5460
    },
    {
      "epoch": 1.952495704754892,
      "grad_norm": 3.867992877960205,
      "learning_rate": 1.2223411848679515e-06,
      "loss": 2.3559,
      "step": 5470
    },
    {
      "epoch": 1.9560657786107951,
      "grad_norm": 3.3758442401885986,
      "learning_rate": 1.1331192005710207e-06,
      "loss": 2.4103,
      "step": 5480
    },
    {
      "epoch": 1.959635852466698,
      "grad_norm": 2.797703981399536,
      "learning_rate": 1.04389721627409e-06,
      "loss": 2.4117,
      "step": 5490
    },
    {
      "epoch": 1.9632059263226007,
      "grad_norm": 2.976860761642456,
      "learning_rate": 9.546752319771592e-07,
      "loss": 2.3328,
      "step": 5500
    }
  ],
  "logging_steps": 10,
  "max_steps": 5604,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.001512551224115e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
