Micron has been written up several times so please refer to those for business/industry primer. I will focus on the AI part of the investment thesis and how HBM (high bandwidth memory) will benefit both Micron’s emerging AI business and its legacy DRAM business.
The growth of HBM will restrict supply in its legacy DRAM business boosting ASPs. HBM has a trade ratio of 3:1, it takes 3x as many wafers to produce the same bit as standard products in the technology nodes. Therefore, we are likely entering into the mother of all memory cycles.
“As discussed previously, the ramp of HBM production will constrain supply growth in non-HBM products. Industry-wide, HBM3E consumes approximately 3x the wafer supply as DDR5 (DRAM) to produce a given number of bits in the same technology node. With increased performance and packaging complexity across the industry, we expect the trade ratio for HBM4 to be even higher than the trade ratio for HBM3E. We anticipate strong HBM demand due to AI, combined with increasing silicon intensity of the HBM roadmap, to contribute to tight supply conditions for DRAM across all end-markets.”
“So on your question regarding wafer shift to HBM, as we have highlighted that HBM3E needs 3x more wafers than -- nearly 3x more wafers than DDR5 in the same technology node of the same capacity to produce the same bits. So this is, of course, highly silicon-intensive technology. And this factor of 3 as a trade ratio between HBM and D5 is really common across the industry. So what -- and HBM demand is increasing rapidly. You see all the recent announcements that are only showing you that even greater attach rate of HBM to the latest GPU solutions that were just announced earlier this week, 192 gigabyte in the Blackwell platforms versus 144 gigabytes. And of course, this is a phenomenon that's occurring across the board. Even today, I think Broadcom talked about how HBM content is going to further increase. So HBM is in a high demand growth phase, and this demand growth will continue in terms of bits, in terms of revenue over the course of foreseeable future. And this is putting tremendous pressure on the non-HBM supply. The trade ratio of 3:1, increasing demand in HBM, increased profitability of HBM is putting a non-HBM part of the memory in tight supply. This is why we say that leading-edge nodes are in very tight supply. And as a result, we would fully expect that D5 as well as other DDR products will improve in their profitability picture as well, given they're very much tight supply there. And of course, HBM being in a strong position, when you look at the LTAs, we have talked to you about our supply already being locked up for '24 and '25. And this then increases our confidence in our D5 as well as LP5, LTA positions with the customers.”
High bandwidth memory (HBM) has become a key enabler of the wave of innovation in LLMs by providing GPUs with high speed access to stacks of co-packaged DRAM, speeding up both AI training and inferencing.
Cloud-native workloads are drivers of growth through use-cases like intelligent edge devices capable of AI and augmented reality that store and access data in the cloud or rely on the cloud for compute capability. Cloud servers supporting AI and data-centric workloads require significantly increasing quantities of DRAM, HBM, and NAND as the task of turning data into insight becomes increasingly memory-centric. As modern servers pack more processing cores into CPUs, the memory bandwidth per CPU core has been decreasing. Micron’s DDR5 alleviates this bottleneck by providing higher bandwidth compared to previous generations, enabling improved performance and scaling. We expect that Micron’s new server DDR5 memory will be a key enabler of CPU core count growth and the bandwidth that DDR5 delivers will be central to unlocking overall server system performance gains for data-intensive workloads like AI and high-performance computing.
NVIDIA’s newly announced next-generation Blackwell GPU architecture-based AI systems provides a 33% increase in HBM3E content, continuing a trend of steadily increasing HBM content per GPU. We expect next-gen AI PC units to grow and become a meaningful portion of total PC units in calendar 2025. We expect AI phones to carry 50 to 100% greater DRAM content compared to non-AI flagship phones today.
Micron’s HBM capacity is completely sold out for the coming two years.
“As we have said before, our HBM is sold out for calendar 2024 and 2025 with pricing already determined for this time frame. In calendar 2025 and 2026, we will have a more diversified HBM revenue profile as we have won business across a broad range of HBM customers with our industry-leading HBM3E solution.”
The HBM TAM and overall opportunity is growing rapidly.
Micron’s HBM provides a 30% benefit on power consumption which is extremely important because power has become a major bottleneck for AI data center buildouts.
“In HBM, we are making excellent progress on our yield and output capability. In fiscal Q4, we delivered on our expected volumes and achieved our objective of several hundred millions of dollars in revenue from HBM in fiscal year 2024. Even as our DRAM gross margins improved, our fiscal Q4 HBM gross margins were accretive to both company and DRAM gross margins indicative of our solid HBM yield ramp. We expect to achieve HBM market share commensurate with our overall DRAM market share sometime in calendar 2025. We expect the HBM TAM to grow from approximately $4 billion in calendar 2023 to over $25 billion in calendar 2025. As a percent of overall industry DRAM base, we expect HBM to grow from 1.5% in calendar 2023 to around 6% in calendar 2025. We have a robust road map for HBM and are confident we will maintain our time-to-market, technology and power efficiency leadership with HBM4 and HBM4E.”
“We have the industry's best HBM3E product, and it's the best product with 30% lower power with 8-high. And in fact, when you go to 12-high, we are 20% lower power despite 50% increase in capacity versus others, 8 high products. So we are well positioned with our product, with its performance, with this power. And that's what is really putting us in the strong position of being -- product being sold out for our '24 and '25 time frame. And when we look at HBM, we have talked about that next year, we project a TAM of $25 billion, consuming about 6%, over 6% of the industry [ base ], in fact, a TAM of greater than $25 billion in 2025. And we are pretty confident that with our product, with our yield ramp, and with the agreement that we have in place with our customers, we will deliver sometime in 2025, get to our share to be in line with our industry share. So of course, it's limited at this point by our production ramp, but we are really on a very good trajectory there. So we feel very confident with our product and with the production ramp and with share opportunities. And frankly, our HBM3E product is getting premium in the industry as well versus other products. So it just puts us on a good trajectory ahead as well.”
Given the HBM tailwinds and its positive impact on the rest of Micron’s ASPs, we model continued high revenue growth in 2025 and 2026, before Micron reverts to its long-run growth profile.
HBM remains accretive to both DRAM and overall company gross margins as it’s a high value solution. We have gross margins expanding to 53% in 2026 and with economies of scale, that should drop down nicely to the bottom line – we model 38% EBIT margins in 2026.
While Micron has historically operated in a boom/bust industry, the industry has consolidated significantly and gotten more rational (profitable, durable growth vs. growth/market share at all cost). DRAM especially is more of an oligopoly now. The end demand drivers are also more diverse: such as Data Centers and EVs. Therefore, Micron should be able to trade at a higher valuation multiple of normalized earnings instead of on book value.
Furthermore, the last earnings should have cleared the way for investors to stay involved as HBM3E continues to ramp, making Micron an AI play. The commentary on the last earnings call should have also invalidated the bear thesis around a prolonged DRAM winter.
Mizuho: “MU sees much better pricing and margins from (1) strong and improving HBM yields, (2) stronger demand and pricing from cloud and server data center demand, (3) less bad PC and smartphone end markets and outlook into 2025 with acceleration expected in PC in 2H25.” AI-driven enterprise SSD (eSSD) now represents 50% of the NAND in the Q, up from only 20% two quarters ago.
Micron has yet to receive an AI premium even though a higher than historical multiple becomes easier to justify – we are at the start of a memory cycle that will be more powerful than previous ones given the strong ramp in HBM and AI demand (both training and inference/AI at the edge). We think Micron can do $15.8 of EPS in 2026, higher than the $12 of EPS they printed in 2018 at the prior cycles peak. 12x 2026 eps of $15.8 results in 85% upside for a 36% IRR.